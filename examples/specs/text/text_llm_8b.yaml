metagen_version: "1.0"
name: "text_llm_8b"
description: "General-purpose text LLM tuned for long-form reasoning."
modality:
  inputs: ["text"]
  outputs: ["text"]
task:
  type: "generation"
  domain: "text"
constraints:
  latency: "near-real-time"
  device: "consumer_gpu"
  parameter_budget:
    max: "8B"
  memory_budget: "16GB"
  context_window: "256k"
  throughput: "120 tok/s"
training:
  objective: ["autoregressive"]
  data:
    sources: ["scraped", "licensed", "synthetic"]
    size: "2T tokens"
    governance:
      pii: "filtered"
      copyright: "mostly"
  compute:
    hardware: "256xA100"
    duration: "21 days"
  alignment:
    method: ["rlhf", "rlaif"]
    policy: "helpful-harmless-ish"
architecture:
  family: "transformer"
  components:
    - name: "SpecEncoder"
      type: "transformer_encoder"
    - name: "ModelLatent"
      type: "hypernetwork_latent"
    - name: "ArchitectureSynth"
      type: "graph_generator"
    - name: "LossComposer"
      type: "objective_mixer"
    - name: "PaperHead"
      type: "latex_decoder"
