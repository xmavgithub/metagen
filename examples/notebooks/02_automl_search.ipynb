{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoML Architecture Search\n",
    "\n",
    "This notebook demonstrates MetaGen's AutoML capabilities for discovering optimal architectures.\n",
    "\n",
    "## Overview\n",
    "\n",
    "MetaGen can automatically search for architectures that balance:\n",
    "- **Parameters**: Stay within budget\n",
    "- **Latency**: Meet inference requirements\n",
    "- **Performance**: Maximize capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from metagen.automl.objectives import compute_pareto_front\n",
    "from metagen.automl.search_engine import ArchitectureSearchEngine\n",
    "from metagen.specs.loader import load_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load a Spec\n",
    "\n",
    "Start with a text LLM spec as our optimization target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spec\n",
    "spec_path = Path(\"../specs/text_llm_8b.yaml\")\n",
    "spec, seed = load_spec(spec_path)\n",
    "\n",
    "print(f\"Spec: {spec.name}\")\n",
    "print(f\"Target parameter budget: {spec.constraints.parameter_budget.max}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Search\n",
    "\n",
    "Quick exploration with random sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create search engine\n",
    "engine = ArchitectureSearchEngine(spec=spec, seed=42)\n",
    "\n",
    "# Run random search\n",
    "results = engine.search(budget=20, strategy=\"random\")\n",
    "\n",
    "print(f\"Searched {len(results.candidates)} candidates\")\n",
    "print(\"\\nTop 5 candidates:\")\n",
    "for i, c in enumerate(results.top_k(5), 1):\n",
    "    print(\n",
    "        f\"  {i}. Score: {c.score:.3f}, Params: {c.params_billion:.2f}B, \"\n",
    "        f\"Hidden: {c.hidden_size}, Layers: {c.num_layers}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evolutionary Search\n",
    "\n",
    "Better results with evolutionary optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create engine with evolution strategy\n",
    "engine_evo = ArchitectureSearchEngine(spec=spec, seed=42)\n",
    "\n",
    "# Run evolutionary search\n",
    "results_evo = engine_evo.search(budget=50, strategy=\"evolution\", generations=5, population_size=10)\n",
    "\n",
    "print(\"Evolutionary search complete\")\n",
    "print(\"\\nTop 5 candidates:\")\n",
    "for i, c in enumerate(results_evo.top_k(5), 1):\n",
    "    print(\n",
    "        f\"  {i}. Score: {c.score:.3f}, Params: {c.params_billion:.2f}B, \"\n",
    "        f\"Hidden: {c.hidden_size}, Layers: {c.num_layers}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare Search Strategies\n",
    "\n",
    "Let's compare the best results from both strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best from each strategy\n",
    "best_random = results.top_k(1)[0]\n",
    "best_evo = results_evo.top_k(1)[0]\n",
    "\n",
    "print(\"Comparison:\")\n",
    "print(f\"{'Metric':<20} {'Random':<15} {'Evolution':<15}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Score':<20} {best_random.score:<15.3f} {best_evo.score:<15.3f}\")\n",
    "print(f\"{'Params (B)':<20} {best_random.params_billion:<15.2f} {best_evo.params_billion:<15.2f}\")\n",
    "print(f\"{'Hidden Size':<20} {best_random.hidden_size:<15} {best_evo.hidden_size:<15}\")\n",
    "print(f\"{'Layers':<20} {best_random.num_layers:<15} {best_evo.num_layers:<15}\")\n",
    "print(f\"{'Heads':<20} {best_random.num_heads:<15} {best_evo.num_heads:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pareto Front Analysis\n",
    "\n",
    "Find architectures that are optimal trade-offs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Pareto front\n",
    "pareto = compute_pareto_front(results_evo.candidates)\n",
    "\n",
    "print(f\"Pareto-optimal candidates: {len(pareto)}\")\n",
    "print(\"\\nPareto front (non-dominated solutions):\")\n",
    "for i, c in enumerate(pareto[:5], 1):\n",
    "    print(\n",
    "        f\"  {i}. Params: {c.params_billion:.2f}B, \"\n",
    "        f\"Latency: {c.latency_ms:.0f}ms, Score: {c.score:.3f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Results\n",
    "\n",
    "Plot the search results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract data\n",
    "params = [c.params_billion for c in results_evo.candidates]\n",
    "scores = [c.score for c in results_evo.candidates]\n",
    "latencies = [c.latency_ms for c in results_evo.candidates]\n",
    "\n",
    "# Pareto front data\n",
    "pareto_params = [c.params_billion for c in pareto]\n",
    "pareto_scores = [c.score for c in pareto]\n",
    "\n",
    "# Create figure\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Params vs Score\n",
    "axes[0].scatter(params, scores, alpha=0.6, label=\"All candidates\")\n",
    "axes[0].scatter(pareto_params, pareto_scores, c=\"red\", s=100, marker=\"*\", label=\"Pareto front\")\n",
    "axes[0].set_xlabel(\"Parameters (B)\")\n",
    "axes[0].set_ylabel(\"Score\")\n",
    "axes[0].set_title(\"Parameters vs Score\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Params vs Latency\n",
    "axes[1].scatter(params, latencies, alpha=0.6, c=scores, cmap=\"viridis\")\n",
    "axes[1].set_xlabel(\"Parameters (B)\")\n",
    "axes[1].set_ylabel(\"Latency (ms)\")\n",
    "axes[1].set_title(\"Parameters vs Latency (color = score)\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Best Architecture\n",
    "\n",
    "Save the best architecture for synthesis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best candidate\n",
    "best = results_evo.top_k(1)[0]\n",
    "\n",
    "# Create architecture config\n",
    "architecture_config = {\n",
    "    \"hidden_size\": best.hidden_size,\n",
    "    \"num_layers\": best.num_layers,\n",
    "    \"num_heads\": best.num_heads,\n",
    "    \"ffn_multiplier\": best.ffn_multiplier,\n",
    "    \"estimated_params_billion\": best.params_billion,\n",
    "    \"estimated_latency_ms\": best.latency_ms,\n",
    "    \"optimization_score\": best.score,\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "output_path = Path(\"./outputs/best_architecture.json\")\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(architecture_config, f, indent=2)\n",
    "\n",
    "print(f\"Best architecture saved to: {output_path}\")\n",
    "print(json.dumps(architecture_config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Search with Different Constraints\n",
    "\n",
    "Let's try a smaller model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tiny spec\n",
    "tiny_spec_path = Path(\"../specs/text_llm_tiny.yaml\")\n",
    "tiny_spec, tiny_seed = load_spec(tiny_spec_path)\n",
    "\n",
    "print(f\"Tiny spec target: {tiny_spec.constraints.parameter_budget.max}\")\n",
    "\n",
    "# Search\n",
    "tiny_engine = ArchitectureSearchEngine(spec=tiny_spec, seed=42)\n",
    "tiny_results = tiny_engine.search(budget=20, strategy=\"evolution\", generations=3)\n",
    "\n",
    "print(\"\\nTop 3 tiny architectures:\")\n",
    "for i, c in enumerate(tiny_results.top_k(3), 1):\n",
    "    print(\n",
    "        f\"  {i}. Params: {c.params_billion * 1000:.0f}M, \"\n",
    "        f\"Hidden: {c.hidden_size}, Layers: {c.num_layers}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- [03_multi_modal.ipynb](03_multi_modal.ipynb) - Different modalities\n",
    "- [AutoML Guide](../../docs/user-guide/automl_guide.md) - Complete reference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
