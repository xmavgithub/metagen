{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Modal Models with MetaGen\n",
    "\n",
    "This notebook demonstrates synthesizing models for different modalities:\n",
    "- Text (LLMs)\n",
    "- Image (ViT, Diffusion)\n",
    "- Audio (MusicGen-style)\n",
    "- Video (Generation)\n",
    "- Multi-modal (CLIP-style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pathlib import Path\n",
    "\n",
    "import yaml\n",
    "\n",
    "from metagen.specs.loader import load_spec\n",
    "from metagen.synth.engine import synthesize\n",
    "from metagen.synth.modalities import get_handler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text Modality\n",
    "\n",
    "Standard transformer-based LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load text spec\n",
    "text_spec, text_seed = load_spec(Path(\"../specs/text/text_llm_8b.yaml\"))\n",
    "\n",
    "print(f\"Text Model: {text_spec.name}\")\n",
    "print(f\"  Inputs: {text_spec.modality.inputs}\")\n",
    "print(f\"  Outputs: {text_spec.modality.outputs}\")\n",
    "print(f\"  Architecture: {text_spec.architecture.family}\")\n",
    "print(f\"  Objective: {text_spec.training.objective}\")\n",
    "\n",
    "# Get handler\n",
    "handler = get_handler(text_spec)\n",
    "print(f\"  Handler: {handler.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Image Modality - Vision Transformer\n",
    "\n",
    "ViT for image classification/encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ViT spec\n",
    "vit_spec, vit_seed = load_spec(Path(\"../specs/image/image_vit_base.yaml\"))\n",
    "\n",
    "print(f\"Vision Transformer: {vit_spec.name}\")\n",
    "print(f\"  Inputs: {vit_spec.modality.inputs}\")\n",
    "print(f\"  Outputs: {vit_spec.modality.outputs}\")\n",
    "print(f\"  Architecture: {vit_spec.architecture.family}\")\n",
    "print(f\"  Parameter budget: {vit_spec.constraints.parameter_budget.max}\")\n",
    "\n",
    "# Get handler\n",
    "handler = get_handler(vit_spec)\n",
    "print(f\"  Handler: {handler.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Image Modality - Diffusion Model\n",
    "\n",
    "U-Net based diffusion for image generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load diffusion spec\n",
    "diff_spec, diff_seed = load_spec(Path(\"../specs/image/image_diffusion_sdxl_like.yaml\"))\n",
    "\n",
    "print(f\"Diffusion Model: {diff_spec.name}\")\n",
    "print(f\"  Inputs: {diff_spec.modality.inputs}\")\n",
    "print(f\"  Outputs: {diff_spec.modality.outputs}\")\n",
    "print(f\"  Architecture: {diff_spec.architecture.family}\")\n",
    "print(f\"  Objective: {diff_spec.training.objective}\")\n",
    "print(f\"  Parameter budget: {diff_spec.constraints.parameter_budget.max}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Audio Modality\n",
    "\n",
    "MusicGen-style audio generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio spec\n",
    "audio_spec, audio_seed = load_spec(Path(\"../specs/audio/audio_musicgen.yaml\"))\n",
    "\n",
    "print(f\"Audio Model: {audio_spec.name}\")\n",
    "print(f\"  Inputs: {audio_spec.modality.inputs}\")\n",
    "print(f\"  Outputs: {audio_spec.modality.outputs}\")\n",
    "print(f\"  Architecture: {audio_spec.architecture.family}\")\n",
    "print(f\"  Context window: {audio_spec.constraints.context_window}\")\n",
    "\n",
    "# Get handler\n",
    "handler = get_handler(audio_spec)\n",
    "print(f\"  Handler: {handler.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Video Modality\n",
    "\n",
    "Video generation model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load video spec\n",
    "video_spec, video_seed = load_spec(Path(\"../specs/video/video_generation.yaml\"))\n",
    "\n",
    "print(f\"Video Model: {video_spec.name}\")\n",
    "print(f\"  Inputs: {video_spec.modality.inputs}\")\n",
    "print(f\"  Outputs: {video_spec.modality.outputs}\")\n",
    "print(f\"  Architecture: {video_spec.architecture.family}\")\n",
    "\n",
    "# Get handler\n",
    "handler = get_handler(video_spec)\n",
    "print(f\"  Handler: {handler.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multi-Modal - CLIP Style\n",
    "\n",
    "Contrastive learning across text and images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load multimodal spec\n",
    "clip_spec, clip_seed = load_spec(Path(\"../specs/multimodal/multimodal_clip.yaml\"))\n",
    "\n",
    "print(f\"Multi-Modal Model: {clip_spec.name}\")\n",
    "print(f\"  Inputs: {clip_spec.modality.inputs}\")\n",
    "print(f\"  Outputs: {clip_spec.modality.outputs}\")\n",
    "print(f\"  Architecture: {clip_spec.architecture.family}\")\n",
    "print(f\"  Objective: {clip_spec.training.objective}\")\n",
    "\n",
    "# Get handler\n",
    "handler = get_handler(clip_spec)\n",
    "print(f\"  Handler: {handler.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Synthesize Different Modalities\n",
    "\n",
    "Let's synthesize models for each modality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthesize a selection of models\n",
    "specs_to_synth = [\n",
    "    (\"../specs/text/text_llm_tiny.yaml\", \"text_tiny\"),\n",
    "    (\"../specs/image/image_vit_base.yaml\", \"image_vit\"),\n",
    "    (\"../specs/multimodal/multimodal_clip.yaml\", \"multimodal_clip\"),\n",
    "]\n",
    "\n",
    "results = {}\n",
    "for spec_path, name in specs_to_synth:\n",
    "    spec, seed = load_spec(Path(spec_path))\n",
    "    output_dir = Path(f\"./outputs/multi_modal/{name}\")\n",
    "\n",
    "    result = synthesize(spec=spec, output_dir=output_dir, seed=seed)\n",
    "    results[name] = result\n",
    "\n",
    "    print(f\"Synthesized: {name}\")\n",
    "    print(f\"  Output: {result['output_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compare Architectures\n",
    "\n",
    "Let's compare the generated architectures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{'Model':<20} {'Hidden':<10} {'Layers':<10} {'Heads':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for name, result in results.items():\n",
    "    arch_path = Path(result[\"output_dir\"]) / \"blueprint\" / \"architecture.yaml\"\n",
    "    with open(arch_path) as f:\n",
    "        arch = yaml.safe_load(f)\n",
    "\n",
    "    hidden = arch.get(\"hidden_size\", \"N/A\")\n",
    "    layers = arch.get(\"num_layers\", \"N/A\")\n",
    "    heads = arch.get(\"num_heads\", \"N/A\")\n",
    "\n",
    "    print(f\"{name:<20} {str(hidden):<10} {str(layers):<10} {str(heads):<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Examine Model Card Differences\n",
    "\n",
    "Each modality gets appropriate documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, result in results.items():\n",
    "    model_card_path = Path(result[\"output_dir\"]) / \"docs\" / \"model_card.md\"\n",
    "    with open(model_card_path) as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # Show first 20 lines\n",
    "    lines = content.split(\"\\n\")[:20]\n",
    "\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Model Card: {name}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    for line in lines:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Create a Custom Multi-Modal Spec\n",
    "\n",
    "Let's create a custom vision-language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metagen.specs.schema import (\n",
    "    Architecture,\n",
    "    Constraints,\n",
    "    Modality,\n",
    "    ModelSpec,\n",
    "    ParameterBudget,\n",
    "    Training,\n",
    ")\n",
    "\n",
    "# Create custom spec programmatically\n",
    "custom_spec = ModelSpec(\n",
    "    metagen_version=\"1.0\",\n",
    "    name=\"custom_vlm\",\n",
    "    description=\"Custom vision-language model for image captioning.\",\n",
    "    modality=Modality(inputs=[\"text\", \"image\"], outputs=[\"text\"]),\n",
    "    constraints=Constraints(\n",
    "        parameter_budget=ParameterBudget(max=\"3B\"), latency=\"near-real-time\", device=\"consumer_gpu\"\n",
    "    ),\n",
    "    training=Training(objective=[\"autoregressive\"]),\n",
    "    architecture=Architecture(family=\"transformer\"),\n",
    ")\n",
    "\n",
    "print(f\"Custom spec: {custom_spec.name}\")\n",
    "print(f\"  Modality: {custom_spec.modality.inputs} â†’ {custom_spec.modality.outputs}\")\n",
    "\n",
    "# Synthesize\n",
    "custom_result = synthesize(\n",
    "    spec=custom_spec, output_dir=Path(\"./outputs/multi_modal/custom_vlm\"), seed=42\n",
    ")\n",
    "\n",
    "print(f\"  Output: {custom_result['output_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available Modality Handlers\n",
    "\n",
    "Summary of MetaGen's modality support:\n",
    "\n",
    "| Handler | Modality | Architectures |\n",
    "|---------|----------|---------------|\n",
    "| TextModalityHandler | text | transformer |\n",
    "| ImageModalityHandler | image | transformer, cnn, diffusion, hybrid |\n",
    "| AudioModalityHandler | audio | transformer |\n",
    "| VideoModalityHandler | video | transformer, diffusion |\n",
    "| MultimodalHandler | text+image | transformer, hybrid |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "- [Multi-Modal Guide](../../docs/user-guide/multi_modal.md) - Complete reference\n",
    "- [Spec Language](../../docs/user-guide/spec_language.md) - All spec options"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
