# Auto-generated by MetaGen; deterministic given spec + seed.
import json
import math
import time
import sys
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
except ImportError:
    torch = None

try:
    from tqdm import tqdm
except ImportError:
    def tqdm(x, **kwargs): return x

def train(
    model,
    data_loader,
    epochs: int = 1,
    lr: float = 1e-4,
    device: str = None,
    log_interval: int = 10,
    max_steps: int | None = None,
    early_stop_threshold: float = 1e6,
):
    """
    Training loop for {{ spec.name }}.
    """
    if not torch:
        print("PyTorch not installed. Skipping training.")
        return {
            "final_loss": 0.0,
            "steps": 0,
            "early_stopped": True,
            "device": "none",
            "runtime_sec": 0.0,
        }

    # Device selection
    if device is None:
        if torch.cuda.is_available():
            device = "cuda"
        elif torch.backends.mps.is_available():
            device = "mps"
        else:
            device = "cpu"
    
    print(f"Training on {device}...", flush=True)
    device = torch.device(device)
    model = model.to(device)
    model.train()

    # Optimizer
    optimizer = optim.AdamW(model.parameters(), lr=lr)
    
    # Loss function
    {% if task_components and task_components.loss_type %}
    # Task-specific loss
    {% if task_components.loss_type == "cross_entropy" %}
    criterion = nn.CrossEntropyLoss()
    {% elif task_components.loss_type == "cross_entropy_with_label_smoothing" %}
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
    {% elif task_components.loss_type == "mse" %}
    criterion = nn.MSELoss()
    {% else %}
    # Default fallback for unknown task loss
    criterion = nn.MSELoss()
    {% endif %}
    {% elif blueprint.vocab_size %}
    criterion = nn.CrossEntropyLoss()
    {% else %}
    criterion = nn.MSELoss()
    {% endif %}

    start_time = time.time()
    
    total_steps = 0
    early_stopped = False
    last_loss = 0.0

    for epoch in range(epochs):
        epoch_loss = 0.0
        batches = 0
        
        for batch_idx, batch in enumerate(data_loader):
            optimizer.zero_grad()
            
            # Handle data structure
            if isinstance(batch, (list, tuple)):
                x = batch[0].to(device)
                y = batch[1].to(device) if len(batch) > 1 else None
            elif isinstance(batch, torch.Tensor):
                x = batch.to(device)
                y = None
            else:
                # Fallback for synthetic/unknown
                continue
                
            # Auto-regressive target shifting
            {% if blueprint.vocab_size %}
            if y is None and x.dim() == 2:
                # Assuming x is [batch, seq_len] indices
                y = x[:, 1:].contiguous()
                x = x[:, :-1].contiguous()
            {% endif %}
                
            # Forward pass
            output = model(x)
            
            # Loss handling
            if isinstance(output, (tuple, list)) and len(output) > 1:
                loss = output[0]  # Assume first output is loss if tuple
            elif hasattr(output, 'loss'):
                loss = output.loss
            else:
                {% if task_components and task_components.loss_type %}
                # Task-specific loss calculation
                if y is not None:
                     loss = criterion(output, y)
                else:
                     raise ValueError(f"Task {{ task_components.head_type }} requires targets (y).")
                {% elif blueprint.vocab_size %}
                # Reshape for CrossEntropy: [B*T, V], [B*T]
                if y is not None:
                    loss = criterion(output.view(-1, output.size(-1)), y.view(-1))
                else:
                    raise ValueError("Vocab-based model requires targets (y) for training. Ensure data loader provides them or auto-regressive shifting works.")
                {% else %}
                if y is not None:
                    loss = criterion(output, y)
                else:
                    # For continuous outputs, if specific targets aren't provided,
                    # we often use reconstruction (MSE against input) or similar self-supervised objective.
                    # But to be explicit and safe:
                    raise ValueError("Model requires targets (y) for training. If self-supervised, ensure data loader yields targets.")
                {% endif %}
            
            # Backward
            loss.backward()
            optimizer.step()
            
            loss_value = float(loss.item())
            last_loss = loss_value
            epoch_loss += loss_value
            batches += 1
            total_steps += 1

            if (not math.isfinite(loss_value)) or loss_value > early_stop_threshold:
                print("Early stopping: loss diverged.", flush=True)
                early_stopped = True
                break
            if max_steps is not None and total_steps >= max_steps:
                break
            
            # Log progress every N batches
            if batches % log_interval == 0:
                print(f"  Epoch {epoch+1} | Batch {batches} | Loss: {loss.item():.4f}", flush=True)
                
        avg_loss = epoch_loss / max(1, batches)
        print(f"Epoch {epoch+1} finished. Avg Loss: {avg_loss:.4f}", flush=True)

        if early_stopped:
            break
        if max_steps is not None and total_steps >= max_steps:
            break

    elapsed = time.time() - start_time
    print(f"Training completed in {elapsed:.2f}s.", flush=True)
    
    return {
        "final_loss": last_loss,
        "steps": total_steps,
        "early_stopped": early_stopped,
        "device": device.type if isinstance(device, torch.device) else str(device),
        "runtime_sec": elapsed,
    }

if __name__ == "__main__":
    import argparse
    import os
    import sys
    
    # Add current dir to path to import generated modules
    script_dir = os.path.dirname(os.path.abspath(__file__))
    sys.path.append(script_dir)
    
    from model import MetaGenModel
    from data import load_data
    
    parser = argparse.ArgumentParser(description="Train MetaGen model")
    parser.add_argument("--data", type=str, default=None, help="Path to training data (.bin or .txt)")
    parser.add_argument("--epochs", type=int, default=1, help="Number of epochs")
    parser.add_argument("--batch-size", type=int, default=4, help="Batch size")
    parser.add_argument("--lr", type=float, default=1e-4, help="Learning rate")
    parser.add_argument(
        "--prototype-mode",
        action="store_true",
        help="Use synthetic prototype data and limited steps.",
    )
    parser.add_argument(
        "--output-metrics",
        type=str,
        default=None,
        help="Path to write JSON metrics for prototype runs.",
    )
    parser.add_argument(
        "--budget-steps",
        type=int,
        default=100,
        help="Max steps for prototype training.",
    )
    parser.add_argument(
        "--early-stop-threshold",
        type=float,
        default=1e6,
        help="Loss threshold for early stopping in prototype mode.",
    )
    args = parser.parse_args()
    
    print("Initializing model...")
    model = MetaGenModel()
    
    print("Loading data...")
    if args.prototype_mode and torch:
        modalities = set({{ spec.modality.inputs + spec.modality.outputs }})
        modalities = {str(m).lower() for m in modalities}
        seq_len = {{ blueprint.max_seq_len or 128 }}
        seq_len = min(128, seq_len)
        hidden_size = {{ blueprint.dims.hidden_size }}
        vocab_size = {{ blueprint.vocab_size or 0 }}
        image_size = min(128, {{ blueprint.image_size or 224 }})
        num_channels = {{ blueprint.num_channels or 3 }}

        def _prototype_loader():
            for _ in range(args.budget_steps):
                if "image" in modalities:
                    yield torch.randn(args.batch_size, num_channels, image_size, image_size)
                elif vocab_size:
                    yield torch.randint(0, vocab_size, (args.batch_size, seq_len))
                else:
                    yield torch.randn(args.batch_size, seq_len, hidden_size)

        loader = _prototype_loader()
    else:
        loader = load_data(data_path=args.data, batch_size=args.batch_size)
    
    print("Starting training...")
    try:
        metrics = train(
            model,
            loader,
            epochs=args.epochs,
            lr=args.lr,
            max_steps=args.budget_steps if args.prototype_mode else None,
            early_stop_threshold=args.early_stop_threshold,
        )
    except KeyboardInterrupt:
        print("Training interrupted.")
        metrics = None

    if args.output_metrics and metrics:
        with open(args.output_metrics, "w", encoding="utf-8") as f:
            json.dump(metrics, f, indent=2)

    if not args.prototype_mode and torch:
        # Save checkpoint in checkpoints/ subfolder
        script_dir = os.path.dirname(os.path.abspath(__file__))
        checkpoints_dir = os.path.join(script_dir, "checkpoints")
        os.makedirs(checkpoints_dir, exist_ok=True)
        checkpoint_path = os.path.join(checkpoints_dir, "checkpoint.pt")
        torch.save(model.state_dict(), checkpoint_path)
        print(f"Checkpoint saved to {checkpoint_path}", flush=True)
