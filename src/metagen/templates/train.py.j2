# Auto-generated by MetaGen; deterministic given spec + seed.
import json
import math
import time
import sys
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
except ImportError:
    torch = None

if torch:
    {% if task_components and task_components.loss_type == "detection_loss" %}
    from model import detection_loss
    {% endif %}
    {% if task_components and task_components.loss_type == "dice_loss" %}
    from model import dice_loss
    {% endif %}
    {% if task_components and task_components.loss_type == "contrastive_loss" %}
    from model import contrastive_loss
    {% endif %}
    {% if task_components and task_components.loss_type in ["policy_gradient_loss", "actor_critic_loss"] %}
    from model import policy_gradient_loss
    {% endif %}
    {% if task_components and task_components.loss_type in ["td_loss", "actor_critic_loss"] %}
    from model import td_loss
    {% endif %}
    {% if task_components and task_components.loss_type == "actor_critic_loss" %}
    from model import actor_critic_loss
    {% endif %}
    {% if task_components and task_components.loss_type == "model_based_loss" %}
    from model import model_based_loss
    {% endif %}
    pass

try:
    from tqdm import tqdm
except ImportError:
    def tqdm(x, **kwargs): return x

def train(
    model,
    data_loader,
    epochs: int = 1,
    lr: float = 1e-4,
    device: str = None,
    log_interval: int = 10,
    max_steps: int | None = None,
    early_stop_threshold: float = 1e6,
):
    """
    Training loop for {{ spec.name }}.
    """
    if not torch:
        print("PyTorch not installed. Skipping training.")
        return {
            "final_loss": 0.0,
            "steps": 0,
            "early_stopped": True,
            "device": "none",
            "runtime_sec": 0.0,
        }

    # Device selection
    if device is None:
        if torch.cuda.is_available():
            device = "cuda"
        elif torch.backends.mps.is_available():
            device = "mps"
        else:
            device = "cpu"
    
    print(f"Training on {device}...", flush=True)
    device = torch.device(device)
    model = model.to(device)
    model.train()

    def _to_device(value):
        if isinstance(value, torch.Tensor):
            return value.to(device)
        if isinstance(value, dict):
            return {k: _to_device(v) for k, v in value.items()}
        if isinstance(value, (list, tuple)):
            return type(value)(_to_device(v) for v in value)
        return value

    # Optimizer
    optimizer = optim.AdamW(model.parameters(), lr=lr)
    
    # Loss function
    {% if task_components and task_components.loss_type %}
    # Task-specific loss
    {% if task_components.loss_type == "cross_entropy" %}
    criterion = nn.CrossEntropyLoss()
    {% elif task_components.loss_type == "cross_entropy_with_label_smoothing" %}
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
    {% elif task_components.loss_type == "bce_with_logits" %}
    criterion = nn.BCEWithLogitsLoss()
    {% elif task_components.loss_type == "mse" %}
    criterion = nn.MSELoss()
    {% else %}
    # Default fallback for unknown task loss
    criterion = nn.MSELoss()
    {% endif %}
    {% elif blueprint.vocab_size %}
    criterion = nn.CrossEntropyLoss()
    {% else %}
    criterion = nn.MSELoss()
    {% endif %}

    start_time = time.time()
    
    total_steps = 0
    early_stopped = False
    last_loss = 0.0

    for epoch in range(epochs):
        epoch_loss = 0.0
        batches = 0
        
        for batch_idx, batch in enumerate(data_loader):
            optimizer.zero_grad()
            
            # Handle data structure
            if isinstance(batch, (list, tuple)):
                x = batch[0]
                y = batch[1] if len(batch) > 1 else None
            elif isinstance(batch, torch.Tensor):
                x = batch
                y = None
            else:
                # Fallback for synthetic/unknown
                continue

            x = _to_device(x)
            y = _to_device(y) if y is not None else None
                
            # Auto-regressive target shifting
            {% if blueprint.vocab_size %}
            if y is None and x.dim() == 2:
                # Assuming x is [batch, seq_len] indices
                y = x[:, 1:].contiguous()
                x = x[:, :-1].contiguous()
            {% endif %}
                
            # Forward pass
            output = model(x)
            
            # Loss handling
            {% if task_components and task_components.loss_type == "detection_loss" %}
            if y is None:
                raise ValueError("Detection tasks require target labels and boxes.")
            if not isinstance(output, dict):
                raise ValueError("Detection output should be a dict with pred_logits/pred_boxes.")
            if isinstance(y, dict):
                target_labels = y.get("labels")
                target_boxes = y.get("boxes")
            elif isinstance(y, (list, tuple)) and len(y) >= 2:
                target_labels, target_boxes = y[0], y[1]
            else:
                raise ValueError("Detection targets must include labels and boxes.")
            loss = detection_loss(
                output["pred_logits"],
                output["pred_boxes"],
                target_labels,
                target_boxes,
            )
            {% elif task_components and task_components.loss_type == "dice_loss" %}
            if y is None:
                raise ValueError("Segmentation tasks require target masks.")
            loss = dice_loss(output, y)
            {% elif task_components and task_components.loss_type == "contrastive_loss" %}
            if y is None:
                raise ValueError("Contrastive tasks require paired inputs.")
            emb_a = output
            emb_b = model(y)
            loss = contrastive_loss(emb_a, emb_b)
            {% elif task_components and task_components.loss_type == "pairwise_ranking_loss" %}
            if y is None:
                raise ValueError("Ranking tasks require paired inputs.")
            score_pos = output
            score_neg = model(y)
            pos = score_pos.view(score_pos.size(0), -1).mean(dim=1)
            neg = score_neg.view(score_neg.size(0), -1).mean(dim=1)
            margin = 1.0
            loss = torch.clamp(margin - (pos - neg), min=0).mean()
            {% elif task_components and task_components.loss_type == "policy_gradient_loss" %}
            if y is None:
                raise ValueError("Policy gradient tasks require actions and advantages.")
            if isinstance(y, dict):
                actions = y.get("actions")
                advantages = y.get("advantages")
            elif isinstance(y, (list, tuple)) and len(y) >= 2:
                actions, advantages = y[0], y[1]
            else:
                raise ValueError("Policy gradient targets must include actions and advantages.")
            logits = output[0] if isinstance(output, (tuple, list)) else output
            {% if (blueprint.action_space or spec.task.action_space or "discrete") == "continuous" %}
            loss = nn.MSELoss()(logits, actions)
            {% else %}
            loss = policy_gradient_loss(logits, actions, advantages)
            {% endif %}
            {% elif task_components and task_components.loss_type == "td_loss" %}
            if y is None:
                raise ValueError("Value-based tasks require target values.")
            targets = y.get("targets") if isinstance(y, dict) else y
            loss = td_loss(output, targets)
            {% elif task_components and task_components.loss_type == "actor_critic_loss" %}
            if y is None:
                raise ValueError("Actor-critic tasks require actions and returns.")
            if isinstance(y, dict):
                actions = y.get("actions")
                advantages = y.get("advantages")
                returns = y.get("returns")
            elif isinstance(y, (list, tuple)) and len(y) >= 3:
                actions, advantages, returns = y[0], y[1], y[2]
            else:
                raise ValueError("Actor-critic targets must include actions, advantages, returns.")
            if isinstance(output, (tuple, list)):
                if len(output) == 3:
                    policy_out, _log_std, value = output
                else:
                    policy_out, value = output
            else:
                raise ValueError("Actor-critic output should include policy and value heads.")
            {% if (blueprint.action_space or spec.task.action_space or "discrete") == "continuous" %}
            policy_loss = nn.MSELoss()(policy_out, actions)
            entropy = torch.zeros((), device=policy_out.device)
            {% else %}
            policy_loss = policy_gradient_loss(policy_out, actions, advantages)
            log_probs = torch.log_softmax(policy_out, dim=-1)
            entropy = -(log_probs * torch.exp(log_probs)).sum(dim=-1).mean()
            {% endif %}
            value = value.view(value.size(0), -1).squeeze(-1)
            returns = returns.view(returns.size(0), -1).squeeze(-1)
            value_loss = td_loss(value, returns)
            loss = actor_critic_loss(policy_loss, value_loss, entropy)
            {% elif task_components and task_components.loss_type == "model_based_loss" %}
            if y is None:
                raise ValueError("Model-based tasks require target next-state tensors.")
            if not isinstance(output, dict) or "next_state" not in output:
                raise ValueError("Model-based output should include next_state.")
            target_state = y.get("next_state") if isinstance(y, dict) else y
            loss = model_based_loss(output["next_state"], target_state)
            {% else %}
            if isinstance(output, (tuple, list)) and len(output) > 1:
                loss = output[0]  # Assume first output is loss if tuple
            elif hasattr(output, "loss"):
                loss = output.loss
            else:
                {% if task_components and task_components.loss_type %}
                # Task-specific loss calculation
                if y is not None:
                    {% if task_components.head_type == "node_classification_head" %}
                    # Node classification expects logits as (batch, classes, nodes)
                    logits = output.permute(0, 2, 1)
                    loss = criterion(logits, y)
                    {% else %}
                    loss = criterion(output, y)
                    {% endif %}
                else:
                    raise ValueError(f"Task {{ task_components.head_type }} requires targets (y).")
                {% elif blueprint.vocab_size %}
                # Reshape for CrossEntropy: [B*T, V], [B*T]
                if y is not None:
                    loss = criterion(output.view(-1, output.size(-1)), y.view(-1))
                else:
                    raise ValueError("Vocab-based model requires targets (y) for training. Ensure data loader provides them or auto-regressive shifting works.")
                {% else %}
                if y is not None:
                    loss = criterion(output, y)
                else:
                    # For continuous outputs, if specific targets aren't provided,
                    # we often use reconstruction (MSE against input) or similar self-supervised objective.
                    # But to be explicit and safe:
                    raise ValueError("Model requires targets (y) for training. If self-supervised, ensure data loader yields targets.")
                {% endif %}
            {% endif %}
            
            # Backward
            loss.backward()
            optimizer.step()
            
            loss_value = float(loss.item())
            last_loss = loss_value
            epoch_loss += loss_value
            batches += 1
            total_steps += 1

            if (not math.isfinite(loss_value)) or loss_value > early_stop_threshold:
                print("Early stopping: loss diverged.", flush=True)
                early_stopped = True
                break
            if max_steps is not None and total_steps >= max_steps:
                break
            
            # Log progress every N batches
            if batches % log_interval == 0:
                print(f"  Epoch {epoch+1} | Batch {batches} | Loss: {loss.item():.4f}", flush=True)
                
        avg_loss = epoch_loss / max(1, batches)
        print(f"Epoch {epoch+1} finished. Avg Loss: {avg_loss:.4f}", flush=True)

        if early_stopped:
            break
        if max_steps is not None and total_steps >= max_steps:
            break

    elapsed = time.time() - start_time
    print(f"Training completed in {elapsed:.2f}s.", flush=True)
    
    return {
        "final_loss": last_loss,
        "steps": total_steps,
        "early_stopped": early_stopped,
        "device": device.type if isinstance(device, torch.device) else str(device),
        "runtime_sec": elapsed,
    }

if __name__ == "__main__":
    import argparse
    import os
    import sys
    
    # Add current dir to path to import generated modules
    script_dir = os.path.dirname(os.path.abspath(__file__))
    sys.path.append(script_dir)
    
    from model import MetaGenModel
    from data import (
        available_sample_data,
        list_remote_datasets,
        suggested_remote_datasets,
        load_data,
    )
    
    parser = argparse.ArgumentParser(description="Train MetaGen model")
    parser.add_argument("--data", type=str, default=None, help="Path to training data (.bin or .txt)")
    parser.add_argument(
        "--dataset",
        type=str,
        default=None,
        help="Remote dataset name (from --list-datasets or hf:<path>).",
    )
    parser.add_argument(
        "--dataset-split",
        type=str,
        default="train",
        help="Split name for remote datasets (default: train).",
    )
    parser.add_argument(
        "--dataset-size",
        type=int,
        default=1024,
        help="Max samples to load from remote datasets (0 = full split).",
    )
    parser.add_argument(
        "--dataset-config",
        type=str,
        default=None,
        help="Optional dataset config name (Hugging Face).",
    )
    parser.add_argument(
        "--dataset-cache-dir",
        type=str,
        default=None,
        help="Optional cache directory for remote datasets.",
    )
    parser.add_argument(
        "--sample-data",
        type=str,
        default=None,
        help="Use a synthetic dataset by name (auto, synthetic_*).",
    )
    parser.add_argument(
        "--sample-size",
        type=int,
        default=256,
        help="Max samples to use for synthetic datasets.",
    )
    parser.add_argument(
        "--list-sample-data",
        action="store_true",
        help="List available sample dataset names and exit.",
    )
    parser.add_argument(
        "--list-datasets",
        action="store_true",
        help="List curated remote datasets and exit.",
    )
    parser.add_argument("--epochs", type=int, default=1, help="Number of epochs")
    parser.add_argument("--batch-size", type=int, default=4, help="Batch size")
    parser.add_argument("--lr", type=float, default=1e-4, help="Learning rate")
    parser.add_argument(
        "--prototype-mode",
        action="store_true",
        help="Use synthetic prototype data and limited steps.",
    )
    parser.add_argument(
        "--output-metrics",
        type=str,
        default=None,
        help="Path to write JSON metrics for prototype runs.",
    )
    parser.add_argument(
        "--budget-steps",
        type=int,
        default=100,
        help="Max steps for prototype training.",
    )
    parser.add_argument(
        "--early-stop-threshold",
        type=float,
        default=1e6,
        help="Loss threshold for early stopping in prototype mode.",
    )
    args = parser.parse_args()

    if args.list_datasets:
        suggested = suggested_remote_datasets()
        if suggested:
            print("Suggested remote datasets for this task:")
            for name in suggested:
                print(f"  - {name}")
        else:
            print("No curated remote datasets for this task. Use --sample-data or hf:<path>.")
        print("Curated remote datasets:")
        for name, desc in list_remote_datasets():
            if desc:
                print(f"  - {name}: {desc}")
            else:
                print(f"  - {name}")
        sys.exit(0)

    if args.list_sample_data:
        print("Available sample datasets:")
        for name in available_sample_data():
            print(f"  - {name}")
        sys.exit(0)

    selections = [bool(args.data), bool(args.dataset), bool(args.sample_data)]
    if sum(selections) > 1:
        print("Error: choose only one of --data, --dataset, or --sample-data.")
        sys.exit(2)

    print("Initializing model...")
    model = MetaGenModel()
    
    print("Loading data...")
    if args.prototype_mode and torch:
        modalities = set({{ spec.modality.inputs + spec.modality.outputs }})
        modalities = {str(m).lower() for m in modalities}
        seq_len = {{ blueprint.max_seq_len or 128 }}
        seq_len = min(128, seq_len)
        hidden_size = {{ blueprint.dims.hidden_size }}
        vocab_size = {{ blueprint.vocab_size or 0 }}
        image_size = min(128, {{ blueprint.image_size or 224 }})
        num_channels = {{ blueprint.num_channels or 3 }}

        def _prototype_loader():
            for _ in range(args.budget_steps):
                if "image" in modalities:
                    yield torch.randn(args.batch_size, num_channels, image_size, image_size)
                elif vocab_size:
                    yield torch.randint(0, vocab_size, (args.batch_size, seq_len))
                else:
                    yield torch.randn(args.batch_size, seq_len, hidden_size)

        loader = _prototype_loader()
    else:
        if args.dataset:
            dataset_size = None if args.dataset_size == 0 else args.dataset_size
            loader = load_data(
                data_path=args.data,
                batch_size=args.batch_size,
                dataset=args.dataset,
                dataset_split=args.dataset_split,
                dataset_size=dataset_size,
                dataset_config=args.dataset_config,
                dataset_cache_dir=args.dataset_cache_dir,
            )
        else:
            sample_size = args.sample_size if (args.sample_data or not args.data) else None
            loader = load_data(
                data_path=args.data,
                batch_size=args.batch_size,
                sample_data=args.sample_data,
                sample_size=sample_size,
            )
    
    print("Starting training...")
    try:
        metrics = train(
            model,
            loader,
            epochs=args.epochs,
            lr=args.lr,
            max_steps=args.budget_steps if args.prototype_mode else None,
            early_stop_threshold=args.early_stop_threshold,
        )
    except KeyboardInterrupt:
        print("Training interrupted.")
        metrics = None

    if args.output_metrics and metrics:
        with open(args.output_metrics, "w", encoding="utf-8") as f:
            json.dump(metrics, f, indent=2)

    if not args.prototype_mode and torch:
        # Save checkpoint in checkpoints/ subfolder
        script_dir = os.path.dirname(os.path.abspath(__file__))
        checkpoints_dir = os.path.join(script_dir, "checkpoints")
        os.makedirs(checkpoints_dir, exist_ok=True)
        checkpoint_path = os.path.join(checkpoints_dir, "checkpoint.pt")
        torch.save(model.state_dict(), checkpoint_path)
        print(f"Checkpoint saved to {checkpoint_path}", flush=True)
