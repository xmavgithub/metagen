# Auto-generated by MetaGen; deterministic given spec + seed.
from __future__ import annotations

import argparse
import sys

try:
    import torch
except ImportError:
    torch = None

EOS_TOKEN_ID = {{ blueprint.eos_token_id or 50256 }}


def generate(
    model,
    prompt_tokens: list[int],
    max_tokens: int = 50,
    device: str = None,
    eos_token_id: int = EOS_TOKEN_ID,
) -> list[int]:
    """
    Autoregressive text generation.
    
    Args:
        model: The model to use for generation
        prompt_tokens: List of token IDs as prompt
        max_tokens: Maximum number of tokens to generate
        device: Device to run on (cuda/mps/cpu)
    
    Returns:
        List of generated token IDs (including prompt)
    """
    if not torch:
        print("PyTorch not installed.")
        return prompt_tokens

    # Device selection
    if device is None:
        if torch.cuda.is_available():
            device = "cuda"
        elif torch.backends.mps.is_available():
            device = "mps"
        else:
            device = "cpu"

    device = torch.device(device)
    model = model.to(device)
    model.eval()

    # Convert to tensor
    tokens = prompt_tokens.copy()
    
    with torch.no_grad():
        for _ in range(max_tokens):
            # Prepare input
            x = torch.tensor([tokens], dtype=torch.long, device=device)
            
            # Forward pass
            output = model(x)
            
            # Get logits for last position
            if isinstance(output, torch.Tensor):
                logits = output[0, -1, :]  # (vocab_size,)
            else:
                logits = output[0][0, -1, :]
            
            # Sample next token (greedy for simplicity)
            next_token = logits.argmax().item()
            tokens.append(next_token)
            
            # Stop if EOS token encountered
            if next_token == eos_token_id:
                break
    
    return tokens


def simple_tokenize(text: str, vocab_size: int = {{ blueprint.vocab_size or 1000 }}) -> list[int]:
    """Simple character-level tokenization (placeholder)."""
    return [ord(c) % vocab_size for c in text]


def simple_detokenize(tokens: list[int]) -> str:
    """Simple character-level detokenization (placeholder)."""
    return "".join(chr(t % 128) for t in tokens if 32 <= t % 128 < 127)


if __name__ == "__main__":
    import os
    script_dir = os.path.dirname(os.path.abspath(__file__))
    sys.path.append(script_dir)
    
    from model import MetaGenModel
    
    parser = argparse.ArgumentParser(description="Run inference with trained model")
    parser.add_argument("--prompt", type=str, default="Hello", help="Input prompt")
    parser.add_argument("--max_tokens", type=int, default=50, help="Max tokens to generate")
    parser.add_argument("--checkpoint", type=str, default=None, help="Checkpoint path (default: script_dir/checkpoint.pt)")
    parser.add_argument("--eos-token-id", type=int, default=EOS_TOKEN_ID, help="EOS token ID")
    args = parser.parse_args()
    
    # Default checkpoint path is in <script_dir>/checkpoints/
    checkpoint_path = args.checkpoint or os.path.join(script_dir, "checkpoints", "checkpoint.pt")
    
    print(f"Loading model from {checkpoint_path}...", flush=True)
    model = MetaGenModel()
    
    if os.path.exists(checkpoint_path):
        state_dict = torch.load(checkpoint_path, map_location="cpu", weights_only=True)
        model.load_state_dict(state_dict)
        print("Checkpoint loaded.", flush=True)
    else:
        print(f"Warning: Checkpoint {checkpoint_path} not found. Using random weights.", flush=True)
    
    # Tokenize prompt
    prompt_tokens = simple_tokenize(args.prompt)
    print(f"Prompt: '{args.prompt}' -> {len(prompt_tokens)} tokens", flush=True)
    
    # Generate
    print("Generating...", flush=True)
    output_tokens = generate(
        model,
        prompt_tokens,
        max_tokens=args.max_tokens,
        eos_token_id=args.eos_token_id,
    )
    
    # Detokenize
    output_text = simple_detokenize(output_tokens)
    print(f"\nGenerated ({len(output_tokens)} tokens):")
    print(output_text)
