# Auto-generated by MetaGen; deterministic given spec + seed.

try:
    import torch
    import torch.nn as nn
except Exception:  # pragma: no cover
    torch = None
    nn = object


class MetaGenModel(nn.Module if torch else object):
    """
    Blueprint model for {{ spec.name }}.
    Inputs: {{ spec.modality.inputs }}
    Outputs: {{ spec.modality.outputs }}

    Architecture (from blueprint):
      hidden_size: {{ blueprint.dims.hidden_size }}
      layers: {{ blueprint.dims.layers }}
      heads: {{ blueprint.dims.heads }}
      dropout: {{ dropout }}
    """

    def __init__(
        self,
        hidden_size: int = {{ blueprint.dims.hidden_size }},
        layers: int = {{ blueprint.dims.layers }},
    ):
        if torch:
            super().__init__()
            # Validate heads
            nhead = {{ blueprint.dims.heads }}
            if nhead < 1:
                nhead = max(4, hidden_size // 256)

            {% if blueprint.vocab_size %}
            self.token_emb = nn.Embedding({{ blueprint.vocab_size }}, hidden_size)
            self.pos_emb = nn.Parameter(torch.zeros(1, {{ blueprint.max_seq_len or 1024 }}, hidden_size))
            {% endif %}

            self.layers = nn.ModuleList([
                nn.TransformerEncoderLayer(
                    d_model=hidden_size,
                    nhead=nhead,
                    dropout={{ dropout }},
                    batch_first=True
                )
                for _ in range(layers)
            ])
            self.norm = nn.LayerNorm(hidden_size)
            
            {% if blueprint.vocab_size %}
            self.output_head = nn.Linear(hidden_size, {{ blueprint.vocab_size }}, bias=False)
            {% endif %}
        else:
            self.layers = []
            self.norm = None

    def forward(self, x):
        if not torch:
            raise RuntimeError("PyTorch not installed; MetaGen generates code as text.")
        
        {% if blueprint.vocab_size %}
        b, t = x.size()
        x = self.token_emb(x) + self.pos_emb[:, :t, :]
        {% endif %}

        for layer in self.layers:
            x = layer(x)
        x = self.norm(x)
        {% if blueprint.vocab_size %}
        x = self.output_head(x)
        {% endif %}
        return x
