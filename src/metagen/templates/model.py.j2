# Auto-generated by MetaGen; deterministic given spec + seed.
{% set head_config = task_components.config if task_components else {} %}

try:
    import torch
    import torch.nn as nn
    {% if task_components and task_components.template_fragments %}
    # Task-specific components
    {% for fragment in task_components.template_fragments %}
    {% include "fragments/" + fragment %}
    {% endfor %}
    {% endif %}
except Exception:  # pragma: no cover
    torch = None
    nn = object


class MetaGenModel(nn.Module if torch else object):
    """
    Blueprint model for {{ spec.name }}.
    Inputs: {{ spec.modality.inputs }}
    Outputs: {{ spec.modality.outputs }}

    Architecture (from blueprint):
      hidden_size: {{ blueprint.dims.hidden_size }}
      layers: {{ blueprint.dims.layers }}
      heads: {{ blueprint.dims.heads }}
      dropout: {{ dropout }}
    """

    def __init__(
        self,
        hidden_size: int = {{ blueprint.dims.hidden_size }},
        layers: int = {{ blueprint.dims.layers }},
    ):
        if torch:
            super().__init__()
            # Validate heads
            nhead = {{ blueprint.dims.heads }}
            if nhead < 1:
                nhead = max(4, hidden_size // 256)

            {% if blueprint.vocab_size %}
            self.token_emb = nn.Embedding({{ blueprint.vocab_size }}, hidden_size)
            self.pos_emb = nn.Parameter(torch.zeros(1, {{ blueprint.max_seq_len or 1024 }}, hidden_size))
            {% endif %}

            self.layers = nn.ModuleList([
                nn.TransformerEncoderLayer(
                    d_model=hidden_size,
                    nhead=nhead,
                    dropout={{ dropout }},
                    batch_first=True
                )
                for _ in range(layers)
            ])
            self.norm = nn.LayerNorm(hidden_size)
            
            {% if blueprint.vocab_size %}
            self.output_head = nn.Linear(hidden_size, {{ blueprint.vocab_size }}, bias=False)
            {% endif %}
            
            {% if task_components and task_components.head_type %}
            # Task-specific output head
            {% if task_components.head_type in ["classification_head", "hierarchical_classification_head"] %}
            self.output_head = ClassificationHead(
                hidden_dim=hidden_size,
                num_classes={{ head_config.get("num_classes", blueprint.num_classes or 1000) }},
                dropout={{ head_config.get("dropout", 0.1) }},
            )
            {% elif task_components.head_type == "regression_head" %}
            self.output_head = RegressionHead(
                hidden_dim=hidden_size,
                num_outputs={{ head_config.get("num_outputs", blueprint.num_outputs or 1) }},
                dropout={{ head_config.get("dropout", 0.1) }},
            )
            {% elif task_components.head_type == "embedding_head" %}
            self.output_head = EmbeddingHead(
                hidden_dim=hidden_size,
                embedding_dim={{ head_config.get("embedding_dim", blueprint.embedding_dim or blueprint.dims.hidden_size) }},
                normalize={{ head_config.get("normalize", True) }},
            )
            {% elif task_components.head_type == "ranking_head" %}
            self.output_head = RankingHead(
                hidden_dim=hidden_size,
                dropout={{ head_config.get("dropout", 0.1) }},
            )
            {% elif task_components.head_type == "detection_head" %}
            self.output_head = DetectionHead(
                hidden_dim=hidden_size,
                num_classes={{ head_config.get("num_classes", 80) }},
                num_anchors={{ head_config.get("num_anchors", 9) }},
                dropout={{ head_config.get("dropout", 0.1) }},
            )
            {% elif task_components.head_type == "segmentation_head" %}
            self.output_head = SegmentationHead(
                hidden_dim=hidden_size,
                num_classes={{ head_config.get("num_classes", 21) }},
                mask_resolution={{ head_config.get("mask_resolution", blueprint.image_size or 128) }},
                dropout={{ head_config.get("dropout", 0.1) }},
            )
            {% elif task_components.head_type == "time_series_head" %}
            self.output_head = TimeSeriesHead(
                hidden_dim=hidden_size,
                horizon={{ head_config.get("horizon", blueprint.horizon or 1) }},
                output_dim={{ head_config.get("output_dim", blueprint.num_outputs or 1) }},
                dropout={{ head_config.get("dropout", 0.1) }},
            )
            {% elif task_components.head_type == "rl_policy_head" %}
            self.output_head = RLPolicyHead(
                hidden_dim=hidden_size,
                action_space="{{ head_config.get('action_space', blueprint.action_space or 'discrete') }}",
                num_actions={{ head_config.get("num_actions", blueprint.num_actions or 0) }},
                action_dim={{ head_config.get("action_dim", blueprint.action_dim or 0) }},
                dropout={{ head_config.get("dropout", 0.1) }},
            )
            {% elif task_components.head_type == "rl_value_head" %}
            self.output_head = RLValueHead(
                hidden_dim=hidden_size,
                action_space="{{ head_config.get('action_space', blueprint.action_space or 'discrete') }}",
                num_actions={{ head_config.get("num_actions", blueprint.num_actions or 0) }},
                action_dim={{ head_config.get("action_dim", blueprint.action_dim or 0) }},
                dropout={{ head_config.get("dropout", 0.1) }},
            )
            {% elif task_components.head_type == "rl_actor_critic_head" %}
            self.output_head = RLActorCriticHead(
                hidden_dim=hidden_size,
                action_space="{{ head_config.get('action_space', blueprint.action_space or 'discrete') }}",
                num_actions={{ head_config.get("num_actions", blueprint.num_actions or 0) }},
                action_dim={{ head_config.get("action_dim", blueprint.action_dim or 0) }},
                dropout={{ head_config.get("dropout", 0.1) }},
            )
            {% elif task_components.head_type == "rl_model_head" %}
            self.output_head = RLModelHead(
                hidden_dim=hidden_size,
                action_space="{{ head_config.get('action_space', blueprint.action_space or 'discrete') }}",
                num_actions={{ head_config.get("num_actions", blueprint.num_actions or 0) }},
                action_dim={{ head_config.get("action_dim", blueprint.action_dim or 0) }},
                dropout={{ head_config.get("dropout", 0.1) }},
            )
            {% else %}
            output_dim = {{ head_config.get("num_outputs", head_config.get("num_classes", 1)) }}
            self.output_head = nn.Linear(hidden_size, output_dim)
            {% endif %}
            {% endif %}
        else:
            self.layers = []
            self.norm = None

    def forward(self, x):
        if not torch:
            raise RuntimeError("PyTorch not installed; MetaGen generates code as text.")
        
        {% if blueprint.vocab_size %}
        b, t = x.size()
        x = self.token_emb(x) + self.pos_emb[:, :t, :]
        {% endif %}

        for layer in self.layers:
            x = layer(x)
        x = self.norm(x)
        {% if blueprint.vocab_size or (task_components and task_components.head_type) %}
        x = self.output_head(x)
        {% endif %}
        return x
