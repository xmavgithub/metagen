# Patch embedding fragment (ViT-style).
import torch
import torch.nn as nn


class PatchEmbedding(nn.Module):
    def __init__(
        self,
        img_size: int = {{ blueprint.image_size or 224 }},
        patch_size: int = {{ blueprint.patch_size or 16 }},
        in_chans: int = {{ blueprint.num_channels or 3 }},
        embed_dim: int = {{ blueprint.dims.hidden_size }},
    ) -> None:
        super().__init__()
        self.img_size = img_size
        self.patch_size = patch_size
        self.num_patches = (img_size // patch_size) ** 2
        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.proj(x)
        x = x.flatten(2).transpose(1, 2)
        return x
