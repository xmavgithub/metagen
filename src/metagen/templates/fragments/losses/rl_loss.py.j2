# Reinforcement learning loss fragments.
import torch
import torch.nn.functional as F


def policy_gradient_loss(
    logits: torch.Tensor,
    actions: torch.Tensor,
    advantages: torch.Tensor,
) -> torch.Tensor:
    if logits.dim() < 2 or logits.size(-1) < 1:
        raise ValueError("logits must be at least 2D with a non-empty action dimension.")
    batch_size = logits.size(0)
    if advantages.dim() != 1 or advantages.size(0) != batch_size:
        raise ValueError("advantages must be 1D and match the logits batch size.")
    if actions.dim() > 1:
        if actions.numel() == batch_size and actions.size(-1) == 1:
            actions = actions.view(batch_size)
        else:
            raise ValueError("actions must be a 1D tensor matching the logits batch size.")
    if actions.size(0) != batch_size:
        raise ValueError("actions must match the logits batch size.")
    if actions.is_floating_point():
        actions = actions.to(torch.long)
    elif actions.dtype not in {torch.int64, torch.int32, torch.int16, torch.int8, torch.uint8}:
        raise ValueError("actions must be an integer tensor.")
    if actions.numel() > 0:
        min_action = int(actions.min().item())
        max_action = int(actions.max().item())
        if min_action < 0 or max_action >= logits.size(-1):
            raise ValueError("actions values must be within logits action dimension.")
    log_probs = F.log_softmax(logits, dim=-1)
    selected = log_probs.gather(-1, actions.unsqueeze(-1)).squeeze(-1)
    return -(selected * advantages).mean()


def td_loss(pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
    return F.mse_loss(pred, target)


def actor_critic_loss(
    policy_loss: torch.Tensor,
    value_loss: torch.Tensor,
    entropy: torch.Tensor,
    value_coef: float = 0.5,
    entropy_coef: float = 0.01,
) -> torch.Tensor:
    """Combine policy, value, and entropy losses with configurable coefficients."""
    return policy_loss + value_coef * value_loss - entropy_coef * entropy


def model_based_loss(pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
    return F.mse_loss(pred, target)
