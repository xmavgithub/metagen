# Transformer encoder fragment.
import torch
import torch.nn as nn


class TransformerEncoder(nn.Module):
    def __init__(
        self,
        hidden_size: int = {{ blueprint.dims.hidden_size }},
        layers: int = {{ blueprint.dims.layers }},
        heads: int = {{ blueprint.dims.heads }},
        dropout: float = 0.1,
    ) -> None:
        super().__init__()
        self.layers = nn.ModuleList(
            [
                nn.TransformerEncoderLayer(
                    d_model=hidden_size,
                    nhead=heads,
                    dropout=dropout,
                    batch_first=True,
                )
                for _ in range(layers)
            ]
        )
        self.norm = nn.LayerNorm(hidden_size)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        for layer in self.layers:
            x = layer(x)
        return self.norm(x)
