# Auto-generated by MetaGen; deterministic given spec + seed.
from __future__ import annotations

import logging
import math
import sys
from typing import Any, Dict, Iterable, Optional

try:
    import torch
    import torch.nn as nn
except ImportError:
    torch = None

try:
    from tqdm import tqdm
except ImportError:
    def tqdm(x, **kwargs): return x

def evaluate(
    model: Any, 
    data_loader: Iterable[Any], 
    device: Optional[str] = None
) -> Dict[str, Any]:
    """
    Evaluates the model on the provided data_loader.
    Returns a dictionary of metrics.
    """
    if not torch:
        return {"error": "PyTorch not installed"}

    # Device selection
    if device is None:
        if torch.cuda.is_available():
            device = "cuda"
        elif torch.backends.mps.is_available():
            device = "mps"
        else:
            device = "cpu"
    
    device = torch.device(device)
    model = model.to(device)
    model.eval()
    
    total_loss = 0.0
    total_batches = 0
    
    with torch.no_grad():
        for batch in tqdm(data_loader, desc="Evaluating"):
            # Handle data structure
            if isinstance(batch, (list, tuple)):
                if not batch:
                    logging.warning("Skipping empty batch.")
                    continue
                if not isinstance(batch[0], torch.Tensor):
                    logging.warning(f"Skipping batch with non-tensor first element: {type(batch[0])}")
                    continue
                x = batch[0].to(device)
            elif isinstance(batch, torch.Tensor):
                x = batch.to(device)
            else:
                raise TypeError(f"Unexpected batch type: {type(batch)}. Expected Tensor, list, or tuple.")
            
            output = model(x)
            
            # Loss handling
            # Loss handling
            loss_val = 0.0
            should_count_batch = True
            
            if isinstance(output, (tuple, list)) and len(output) > 1:
                # Assume first output is loss if tuple
                loss_tensor = output[0]
                if isinstance(loss_tensor, torch.Tensor):
                    loss_val = loss_tensor.detach().item()
                else:
                    loss_val = float(loss_tensor)
            elif hasattr(output, 'loss'):
                # HuggingFace style
                loss_tensor = output.loss
                if isinstance(loss_tensor, torch.Tensor):
                     loss_val = loss_tensor.detach().item()
                else:
                     loss_val = float(loss_tensor)
            elif isinstance(output, torch.Tensor):
                if output.dim() == 0:
                    # Scalar loss
                    loss_val = output.detach().item()
                else:
                    # Non-scalar, possibly logits
                    # Check if it looks like loss (floating point, non-negative)
                    if output.is_floating_point() and output.min() >= 0:
                        loss_val = output.detach().mean().item()
                        logging.warning(f"Batch {total_batches+1}: Model output is non-scalar tensor (min={output.min():.2f}), using mean as loss proxy.")
                    else:
                        logging.warning(f"Batch {total_batches+1}: Model output looks like logits (min < 0 or non-float). Skipping loss aggregation.")
                        should_count_batch = False
            else:
                 logging.warning(f"Batch {total_batches+1}: Unknown model output type: {type(output)}. Skipping.")
                 should_count_batch = False

            if should_count_batch:
                total_loss += loss_val
                total_batches += 1
            
    avg_loss = total_loss / max(1, total_batches)
    try:
        perplexity = math.exp(avg_loss)
    except OverflowError:
        perplexity = float('inf')
        
    return {
        "val_loss": avg_loss,
        "perplexity": perplexity,
        "batches": total_batches
    }

if __name__ == "__main__":
    # Self-test block
    import os
    # Add current dir to path to import generated modules
    sys.path.append(os.path.dirname(__file__))
    
    from model import MetaGenModel
    from data import load_data
    
    print("Initializing model...")
    model = MetaGenModel()
    
    print("Loading data...")
    loader = load_data(batch_size=4, seq_len=32)
    
    print("Starting evaluation...")
    metrics = evaluate(model, loader)
    print("Metrics:", metrics)
