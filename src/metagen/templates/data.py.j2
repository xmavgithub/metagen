# Auto-generated by MetaGen; deterministic given spec + seed.
import os
from pathlib import Path

import numpy as np

try:
    import torch
    from torch.utils.data import DataLoader, Dataset
except ImportError:  # pragma: no cover
    torch = None
    Dataset = object
    DataLoader = object


HEAD_TYPE = "{{ task_components.head_type if task_components else '' }}"
LOSS_TYPE = "{{ task_components.loss_type if task_components else '' }}"
TASK_TYPE = "{{ spec.task.type }}"
VOCAB_SIZE = {{ blueprint.vocab_size or 0 }}
HIDDEN_SIZE = {{ blueprint.dims.hidden_size }}
SEQ_LEN = {{ blueprint.max_seq_len or 128 }}
NUM_CLASSES = {{ blueprint.num_classes or spec.task.num_classes or 2 }}
NUM_OUTPUTS = {{ blueprint.num_outputs or spec.task.num_outputs or 1 }}
HORIZON = {{ blueprint.horizon or spec.task.horizon or 1 }}
LOOKBACK = {{ blueprint.lookback or spec.task.lookback or (blueprint.max_seq_len or 128) }}
NUM_ANCHORS = {{ blueprint.num_anchors or spec.task.num_anchors or 9 }}
MASK_RESOLUTION = {{ blueprint.mask_resolution or spec.task.mask_resolution or (blueprint.image_size or 64) }}
ACTION_SPACE = "{{ blueprint.action_space or spec.task.action_space or 'discrete' }}"
NUM_ACTIONS = {{ blueprint.num_actions or spec.task.num_actions or 4 }}
ACTION_DIM = {{ blueprint.action_dim or spec.task.action_dim or 2 }}
EMBEDDING_DIM = {{ blueprint.embedding_dim or spec.task.embedding_dim or blueprint.dims.hidden_size }}
NUM_NODES = 16


INLINE_TEXT_SAMPLE = (
    "To be, or not to be, that is the question.\n"
    "Whether 'tis nobler in the mind to suffer\n"
    "The slings and arrows of outrageous fortune,\n"
    "Or to take arms against a sea of troubles.\n"
)


class SyntheticTextDataset(Dataset):
    """Generates random integers as tokens for testing."""

    def __init__(self, size=1000, seq_len={{ blueprint.max_seq_len or 128 }}):
        self.size = size
        self.seq_len = seq_len

    def __len__(self):
        return self.size

    def __getitem__(self, idx):
        if not torch:
            return []
        # Return random integers as tokens
        # vocab_size: {{ blueprint.vocab_size or 50257 }}
        return torch.randint(0, {{ blueprint.vocab_size or 50257 }}, (self.seq_len,))


class FileTextDataset(Dataset):
    """Loads text from a flat file and performs character-level tokenization (demo)."""

    def __init__(self, file_path, seq_len={{ blueprint.max_seq_len or 128 }}):
        self.seq_len = seq_len
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"Data file {file_path} not found")
        with open(file_path, "r", encoding="utf-8") as f:
            self.text = f.read()
        # Simplified tokenization (char-level for robustness/demo) if no tokenizer
        # Modulo vocab_size to stay in bounds
        vocab_size = {{ blueprint.vocab_size or 50257 }}
        self.data = [ord(c) % vocab_size for c in self.text]

    def __len__(self):
        return max(0, len(self.data) - self.seq_len)

    def __getitem__(self, idx):
        if not torch:
            return []
        chunk = self.data[idx : idx + self.seq_len]
        return torch.tensor(chunk, dtype=torch.long)


class InlineTextDataset(Dataset):
    """Loads text from an inline string for quick demos."""

    def __init__(self, text: str, seq_len={{ blueprint.max_seq_len or 128 }}):
        self.seq_len = seq_len
        vocab_size = {{ blueprint.vocab_size or 50257 }}
        self.data = [ord(c) % vocab_size for c in text]

    def __len__(self):
        return max(0, len(self.data) - self.seq_len)

    def __getitem__(self, idx):
        if not torch:
            return []
        chunk = self.data[idx : idx + self.seq_len]
        return torch.tensor(chunk, dtype=torch.long)


class BinaryTokenDataset(Dataset):
    """Loads pre-tokenized data from a binary file (uint16 format)."""

    def __init__(self, file_path, seq_len={{ blueprint.max_seq_len or 128 }}):
        self.seq_len = seq_len
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"Data file {file_path} not found")
        # Memory-map for efficiency with large files
        self.data = np.memmap(file_path, dtype=np.uint16, mode="r")
        print(f"Loaded {len(self.data):,} tokens from {file_path}")

    def __len__(self):
        return max(0, len(self.data) - self.seq_len)

    def __getitem__(self, idx):
        if not torch:
            return []
        chunk = self.data[idx : idx + self.seq_len].astype(np.int64)
        return torch.from_numpy(chunk)


class LimitedDataset(Dataset):
    """Wrap a dataset and cap its length."""

    def __init__(self, dataset: Dataset, max_length: int):
        self.dataset = dataset
        self.max_length = max(0, min(len(dataset), max_length))

    def __len__(self):
        return self.max_length

    def __getitem__(self, idx):
        return self.dataset[idx]


def _sample_features(
    seq_len: int = SEQ_LEN,
    hidden_size: int = HIDDEN_SIZE,
    vocab_size: int = VOCAB_SIZE,
) -> torch.Tensor:
    if vocab_size:
        return torch.randint(0, vocab_size, (seq_len,), dtype=torch.long)
    return torch.randn(seq_len, hidden_size)


class SyntheticClassificationDataset(Dataset):
    def __init__(
        self,
        size: int = 256,
        seq_len: int = SEQ_LEN,
        hidden_size: int = HIDDEN_SIZE,
        num_classes: int = NUM_CLASSES,
        vocab_size: int = VOCAB_SIZE,
    ):
        self.size = size
        self.seq_len = seq_len
        self.hidden_size = hidden_size
        self.num_classes = max(2, num_classes)
        self.vocab_size = vocab_size

    def __len__(self):
        return self.size

    def __getitem__(self, idx):
        if not torch:
            return []
        x = _sample_features(self.seq_len, self.hidden_size, self.vocab_size)
        y = torch.randint(0, self.num_classes, (), dtype=torch.long)
        return x, y


class SyntheticRegressionDataset(Dataset):
    def __init__(
        self,
        size: int = 256,
        seq_len: int = SEQ_LEN,
        hidden_size: int = HIDDEN_SIZE,
        output_dim: int = NUM_OUTPUTS,
        vocab_size: int = VOCAB_SIZE,
    ):
        self.size = size
        self.seq_len = seq_len
        self.hidden_size = hidden_size
        self.output_dim = max(1, output_dim)
        self.vocab_size = vocab_size

    def __len__(self):
        return self.size

    def __getitem__(self, idx):
        if not torch:
            return []
        x = _sample_features(self.seq_len, self.hidden_size, self.vocab_size)
        y = torch.randn(self.output_dim)
        return x, y


class SyntheticReconstructionDataset(Dataset):
    def __init__(
        self,
        size: int = 256,
        seq_len: int = SEQ_LEN,
        hidden_size: int = HIDDEN_SIZE,
        vocab_size: int = VOCAB_SIZE,
    ):
        self.size = size
        self.seq_len = seq_len
        self.hidden_size = hidden_size
        self.vocab_size = vocab_size

    def __len__(self):
        return self.size

    def __getitem__(self, idx):
        if not torch:
            return []
        x = _sample_features(self.seq_len, self.hidden_size, self.vocab_size)
        y = x.clone() if isinstance(x, torch.Tensor) else x
        return x, y


class SyntheticTimeSeriesDataset(Dataset):
    def __init__(
        self,
        size: int = 256,
        lookback: int = LOOKBACK,
        hidden_size: int = HIDDEN_SIZE,
        horizon: int = HORIZON,
        output_dim: int = NUM_OUTPUTS,
    ):
        self.size = size
        self.lookback = lookback
        self.hidden_size = hidden_size
        self.horizon = max(1, horizon)
        self.output_dim = max(1, output_dim)

    def __len__(self):
        return self.size

    def __getitem__(self, idx):
        if not torch:
            return []
        x = torch.randn(self.lookback, self.hidden_size)
        y = torch.randn(self.horizon, self.output_dim)
        return x, y


class SyntheticDetectionDataset(Dataset):
    def __init__(
        self,
        size: int = 128,
        seq_len: int = SEQ_LEN,
        hidden_size: int = HIDDEN_SIZE,
        num_classes: int = NUM_CLASSES,
        num_anchors: int = NUM_ANCHORS,
        vocab_size: int = VOCAB_SIZE,
    ):
        self.size = size
        self.seq_len = seq_len
        self.hidden_size = hidden_size
        self.num_classes = max(2, num_classes)
        self.num_anchors = max(1, num_anchors)
        self.vocab_size = vocab_size

    def __len__(self):
        return self.size

    def __getitem__(self, idx):
        if not torch:
            return []
        x = _sample_features(self.seq_len, self.hidden_size, self.vocab_size)
        labels = torch.randint(0, self.num_classes, (self.num_anchors,), dtype=torch.long)
        boxes = torch.rand(self.num_anchors, 4)
        return x, {"labels": labels, "boxes": boxes}


class SyntheticSegmentationDataset(Dataset):
    def __init__(
        self,
        size: int = 128,
        seq_len: int = SEQ_LEN,
        hidden_size: int = HIDDEN_SIZE,
        num_classes: int = NUM_CLASSES,
        mask_resolution: int = MASK_RESOLUTION,
        vocab_size: int = VOCAB_SIZE,
    ):
        self.size = size
        self.seq_len = seq_len
        self.hidden_size = hidden_size
        self.num_classes = max(2, num_classes)
        self.mask_resolution = max(8, mask_resolution)
        self.vocab_size = vocab_size

    def __len__(self):
        return self.size

    def __getitem__(self, idx):
        if not torch:
            return []
        x = _sample_features(self.seq_len, self.hidden_size, self.vocab_size)
        mask = torch.randint(
            0,
            self.num_classes,
            (self.mask_resolution, self.mask_resolution),
            dtype=torch.long,
        )
        return x, mask


class SyntheticGraphDataset(Dataset):
    def __init__(
        self,
        size: int = 256,
        num_nodes: int = NUM_NODES,
        hidden_size: int = HIDDEN_SIZE,
        num_classes: int = NUM_CLASSES,
        output_dim: int = NUM_OUTPUTS,
        mode: str = "graph_classification",
    ):
        self.size = size
        self.num_nodes = max(4, num_nodes)
        self.hidden_size = hidden_size
        self.num_classes = max(2, num_classes)
        self.output_dim = max(1, output_dim)
        self.mode = mode

    def __len__(self):
        return self.size

    def __getitem__(self, idx):
        if not torch:
            return []
        x = torch.randn(self.num_nodes, self.hidden_size)
        if self.mode == "node_classification":
            y = torch.randint(0, self.num_classes, (self.num_nodes,), dtype=torch.long)
        elif self.mode == "link_prediction":
            y = torch.randint(0, 2, (self.num_nodes, self.num_nodes), dtype=torch.float32)
        elif self.mode == "recommendation":
            y = torch.randn(self.output_dim)
        else:
            y = torch.randint(0, self.num_classes, (), dtype=torch.long)
        return x, y


class SyntheticPairDataset(Dataset):
    def __init__(
        self,
        size: int = 256,
        seq_len: int = SEQ_LEN,
        hidden_size: int = HIDDEN_SIZE,
        vocab_size: int = VOCAB_SIZE,
    ):
        self.size = size
        self.seq_len = seq_len
        self.hidden_size = hidden_size
        self.vocab_size = vocab_size

    def __len__(self):
        return self.size

    def __getitem__(self, idx):
        if not torch:
            return []
        left = _sample_features(self.seq_len, self.hidden_size, self.vocab_size)
        right = _sample_features(self.seq_len, self.hidden_size, self.vocab_size)
        return left, right


class SyntheticRLDataset(Dataset):
    def __init__(
        self,
        size: int = 256,
        seq_len: int = SEQ_LEN,
        hidden_size: int = HIDDEN_SIZE,
        action_space: str = ACTION_SPACE,
        num_actions: int = NUM_ACTIONS,
        action_dim: int = ACTION_DIM,
        mode: str = "policy_gradient",
        vocab_size: int = VOCAB_SIZE,
    ):
        self.size = size
        self.seq_len = seq_len
        self.hidden_size = hidden_size
        self.action_space = action_space
        self.num_actions = max(2, num_actions)
        self.action_dim = max(1, action_dim)
        self.mode = mode
        self.vocab_size = vocab_size

    def __len__(self):
        return self.size

    def _sample_action(self):
        if self.action_space == "continuous":
            return torch.randn(self.action_dim)
        return torch.randint(0, self.num_actions, (), dtype=torch.long)

    def __getitem__(self, idx):
        if not torch:
            return []
        x = _sample_features(self.seq_len, self.hidden_size, self.vocab_size)
        if self.mode == "value_based":
            target_dim = self.action_dim if self.action_space == "continuous" else self.num_actions
            y = torch.randn(target_dim)
        elif self.mode == "actor_critic":
            y = {
                "actions": self._sample_action(),
                "advantages": torch.randn(()),
                "returns": torch.randn(()),
            }
        elif self.mode == "model_based":
            y = {"next_state": torch.randn(self.hidden_size)}
        else:
            y = {
                "actions": self._sample_action(),
                "advantages": torch.randn(()),
            }
        return x, y


def _find_examples_data_root() -> Path | None:
    for base in (Path.cwd(), Path(__file__).resolve().parent):
        for parent in [base] + list(base.parents):
            candidate = parent / "examples" / "data"
            if candidate.exists():
                return candidate
    return None


def _build_shakespeare_dataset(seq_len: int) -> Dataset:
    data_root = _find_examples_data_root()
    if data_root:
        txt_path = data_root / "input.txt"
        if txt_path.exists():
            return FileTextDataset(str(txt_path), seq_len=seq_len)
        bin_path = data_root / "train.bin"
        if bin_path.exists():
            return BinaryTokenDataset(str(bin_path), seq_len=seq_len)
    return InlineTextDataset(INLINE_TEXT_SAMPLE, seq_len=seq_len)


def _auto_sample_name() -> str:
    if HEAD_TYPE in {
        "classification_head",
        "hierarchical_classification_head",
        "graph_classification_head",
        "node_classification_head",
        "link_prediction_head",
        "recommendation_head",
    }:
        return "synthetic_graph" if "graph" in HEAD_TYPE else "synthetic_classification"
    if HEAD_TYPE == "regression_head":
        return "synthetic_regression"
    if HEAD_TYPE == "time_series_head":
        return "synthetic_time_series"
    if HEAD_TYPE == "detection_head":
        return "synthetic_detection"
    if HEAD_TYPE == "segmentation_head":
        return "synthetic_segmentation"
    if HEAD_TYPE == "embedding_head":
        return "synthetic_pair"
    if HEAD_TYPE == "ranking_head":
        return "synthetic_pair"
    if HEAD_TYPE in {"rl_policy_head", "rl_value_head", "rl_actor_critic_head", "rl_model_head"}:
        return "synthetic_rl"
    if VOCAB_SIZE:
        return "text_shakespeare"
    return "synthetic_reconstruction"


SAMPLE_DATASETS = {
    "text_shakespeare": lambda seq_len: _build_shakespeare_dataset(seq_len),
    "synthetic_text": lambda seq_len: SyntheticTextDataset(size=256, seq_len=seq_len),
    "synthetic_classification": lambda seq_len: SyntheticClassificationDataset(
        size=256, seq_len=seq_len
    ),
    "synthetic_regression": lambda seq_len: SyntheticRegressionDataset(size=256, seq_len=seq_len),
    "synthetic_reconstruction": lambda seq_len: SyntheticReconstructionDataset(
        size=256, seq_len=seq_len
    ),
    "synthetic_time_series": lambda seq_len: SyntheticTimeSeriesDataset(size=256),
    "synthetic_detection": lambda seq_len: SyntheticDetectionDataset(size=128, seq_len=seq_len),
    "synthetic_segmentation": lambda seq_len: SyntheticSegmentationDataset(size=128, seq_len=seq_len),
    "synthetic_graph": lambda seq_len: SyntheticGraphDataset(
        size=256,
        mode=(
            "node_classification"
            if HEAD_TYPE == "node_classification_head"
            else "link_prediction"
            if HEAD_TYPE == "link_prediction_head"
            else "recommendation"
            if HEAD_TYPE == "recommendation_head"
            else "graph_classification"
        ),
    ),
    "synthetic_pair": lambda seq_len: SyntheticPairDataset(size=256, seq_len=seq_len),
    "synthetic_rl": lambda seq_len: SyntheticRLDataset(
        size=256,
        seq_len=seq_len,
        mode=(
            "value_based"
            if LOSS_TYPE == "td_loss"
            else "actor_critic"
            if LOSS_TYPE == "actor_critic_loss"
            else "model_based"
            if LOSS_TYPE == "model_based_loss"
            else "policy_gradient"
        ),
    ),
}


def available_sample_data() -> list[str]:
    return ["auto", *sorted(SAMPLE_DATASETS.keys())]


def load_data(
    data_path: str = None,
    batch_size: int = 2,
    seq_len: int = {{ blueprint.max_seq_len or 128 }},
    sample_data: str | None = None,
    sample_size: int | None = None,
):
    """
    Data loader respecting inputs: {{ spec.modality.inputs }}
    Returns a PyTorch DataLoader (or a generator if PyTorch is missing).

    Supports:
        - .bin files: Pre-tokenized binary data (uint16)
        - .txt files: Character-level tokenization
        - None: Synthetic random data
    """
    if not torch:
        print("PyTorch not found, returning placeholder generator")

        def _gen():
            while True:
                yield "synthetic_batch"

        return _gen()

    if data_path and os.path.exists(data_path) and os.path.isdir(data_path):
        print("Data path is a directory; using sample dataset instead.")
        data_path = None

    if data_path and os.path.exists(data_path):
        if data_path.endswith(".bin"):
            print(f"Loading pre-tokenized data from {data_path}...")
            dataset = BinaryTokenDataset(data_path, seq_len=seq_len)
        else:
            print(f"Loading text data from {data_path}...")
            dataset = FileTextDataset(data_path, seq_len=seq_len)
    else:
        chosen = sample_data or "auto"
        if chosen == "auto":
            chosen = _auto_sample_name()
        if chosen not in SAMPLE_DATASETS:
            print(f"Unknown sample dataset '{chosen}', falling back to auto.")
            chosen = _auto_sample_name()
        print(f"Using sample dataset: {chosen}")
        dataset = SAMPLE_DATASETS[chosen](seq_len)

    if sample_size is not None:
        dataset = LimitedDataset(dataset, sample_size)

    return DataLoader(dataset, batch_size=batch_size, shuffle=True)
