# Auto-generated by MetaGen; deterministic given spec + seed.
import os

import numpy as np

try:
    import torch
    from torch.utils.data import DataLoader, Dataset
except ImportError:  # pragma: no cover
    torch = None
    Dataset = object
    DataLoader = object


class SyntheticTextDataset(Dataset):
    """Generates random integers as tokens for testing."""

    def __init__(self, size=1000, seq_len={{ blueprint.max_seq_len or 128 }}):
        self.size = size
        self.seq_len = seq_len

    def __len__(self):
        return self.size

    def __getitem__(self, idx):
        if not torch:
            return []
        # Return random integers as tokens
        # vocab_size: {{ blueprint.vocab_size or 50257 }}
        return torch.randint(0, {{ blueprint.vocab_size or 50257 }}, (self.seq_len,))


class FileTextDataset(Dataset):
    """Loads text from a flat file and performs character-level tokenization (demo)."""

    def __init__(self, file_path, seq_len={{ blueprint.max_seq_len or 128 }}):
        self.seq_len = seq_len
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"Data file {file_path} not found")
        with open(file_path, "r", encoding="utf-8") as f:
            self.text = f.read()
        # Simplified tokenization (char-level for robustness/demo) if no tokenizer
        # Modulo vocab_size to stay in bounds
        vocab_size = {{ blueprint.vocab_size or 50257 }}
        self.data = [ord(c) % vocab_size for c in self.text]

    def __len__(self):
        return max(0, len(self.data) - self.seq_len)

    def __getitem__(self, idx):
        if not torch:
            return []
        chunk = self.data[idx : idx + self.seq_len]
        return torch.tensor(chunk, dtype=torch.long)


class BinaryTokenDataset(Dataset):
    """Loads pre-tokenized data from a binary file (uint16 format)."""

    def __init__(self, file_path, seq_len={{ blueprint.max_seq_len or 128 }}):
        self.seq_len = seq_len
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"Data file {file_path} not found")
        # Memory-map for efficiency with large files
        self.data = np.memmap(file_path, dtype=np.uint16, mode="r")
        print(f"Loaded {len(self.data):,} tokens from {file_path}")

    def __len__(self):
        return max(0, len(self.data) - self.seq_len)

    def __getitem__(self, idx):
        if not torch:
            return []
        chunk = self.data[idx : idx + self.seq_len].astype(np.int64)
        return torch.from_numpy(chunk)


def load_data(
    data_path: str = None, batch_size: int = 2, seq_len: int = {{ blueprint.max_seq_len or 128 }}
):
    """
    Data loader respecting inputs: {{ spec.modality.inputs }}
    Returns a PyTorch DataLoader (or a generator if PyTorch is missing).

    Supports:
        - .bin files: Pre-tokenized binary data (uint16)
        - .txt files: Character-level tokenization
        - None: Synthetic random data
    """
    if not torch:
        print("PyTorch not found, returning placeholder generator")

        def _gen():
            while True:
                yield "synthetic_batch"

        return _gen()

    if data_path and os.path.exists(data_path):
        if data_path.endswith(".bin"):
            print(f"Loading pre-tokenized data from {data_path}...")
            dataset = BinaryTokenDataset(data_path, seq_len=seq_len)
        else:
            print(f"Loading text data from {data_path}...")
            dataset = FileTextDataset(data_path, seq_len=seq_len)
    else:
        print("No data path provided or file not found. Using synthetic data.")
        dataset = SyntheticTextDataset(size=1000, seq_len=seq_len)

    return DataLoader(dataset, batch_size=batch_size, shuffle=True)

