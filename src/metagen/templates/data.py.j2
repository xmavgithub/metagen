# Auto-generated by MetaGen; deterministic given spec + seed.
from __future__ import annotations
import os
from pathlib import Path

import numpy as np

try:
    import torch
    from torch.utils.data import DataLoader, Dataset
except ImportError:  # pragma: no cover
    torch = None
    Dataset = object
    DataLoader = object

try:
    from datasets import load_dataset
except ImportError:  # pragma: no cover
    load_dataset = None


HEAD_TYPE = "{{ task_components.head_type if task_components else '' }}"
LOSS_TYPE = "{{ task_components.loss_type if task_components else '' }}"
TASK_TYPE = "{{ spec.task.type }}"
VOCAB_SIZE = {{ blueprint.vocab_size or 0 }}
HIDDEN_SIZE = {{ blueprint.dims.hidden_size }}
SEQ_LEN = {{ blueprint.max_seq_len or 128 }}
NUM_CLASSES = {{ blueprint.num_classes or spec.task.num_classes or 2 }}
NUM_OUTPUTS = {{ blueprint.num_outputs or spec.task.num_outputs or 1 }}
HORIZON = {{ blueprint.horizon or spec.task.horizon or 1 }}
LOOKBACK = {{ blueprint.lookback or spec.task.lookback or (blueprint.max_seq_len or 128) }}
NUM_ANCHORS = {{ blueprint.num_anchors or spec.task.num_anchors or 9 }}
MASK_RESOLUTION = {{ blueprint.mask_resolution or spec.task.mask_resolution or (blueprint.image_size or 64) }}
ACTION_SPACE = "{{ blueprint.action_space or spec.task.action_space or 'discrete' }}"
NUM_ACTIONS = {{ blueprint.num_actions or spec.task.num_actions or 4 }}
ACTION_DIM = {{ blueprint.action_dim or spec.task.action_dim or 2 }}
EMBEDDING_DIM = {{ blueprint.embedding_dim or spec.task.embedding_dim or blueprint.dims.hidden_size }}
NUM_NODES = 16
BASE_SEED = {{ blueprint.seed or 42 }}
INPUT_MODALITIES = {
    {% for m in spec.modality.inputs %}"{{ m | lower }}"{% if not loop.last %}, {% endif %}{% endfor %}
}
OUTPUT_MODALITIES = {
    {% for m in spec.modality.outputs %}"{{ m | lower }}"{% if not loop.last %}, {% endif %}{% endfor %}
}


def _seeded_generator(idx: int | None = None, offset: int = 0) -> torch.Generator:
    if not torch:
        raise RuntimeError("PyTorch not installed.")
    seed = BASE_SEED + offset + (int(idx) if idx is not None else 0)
    gen = torch.Generator()
    gen.manual_seed(seed)
    return gen


class SyntheticTextDataset(Dataset):
    """Generates random integers as tokens for testing."""

    def __init__(self, size=1000, seq_len={{ blueprint.max_seq_len or 128 }}):
        self.size = size
        self.seq_len = seq_len

    def __len__(self):
        return self.size

    def __getitem__(self, idx):
        if not torch:
            return []
        # Return random integers as tokens
        # vocab_size: {{ blueprint.vocab_size or 50257 }}
        generator = _seeded_generator(idx)
        return torch.randint(
            0,
            {{ blueprint.vocab_size or 50257 }},
            (self.seq_len,),
            generator=generator,
        )


class FileTextDataset(Dataset):
    """Loads text from a flat file and performs character-level tokenization (demo)."""

    def __init__(self, file_path, seq_len={{ blueprint.max_seq_len or 128 }}):
        self.seq_len = seq_len
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"Data file {file_path} not found")
        with open(file_path, "r", encoding="utf-8") as f:
            self.text = f.read()
        # Simplified tokenization (char-level for robustness/demo) if no tokenizer
        # Modulo vocab_size to stay in bounds
        vocab_size = {{ blueprint.vocab_size or 50257 }}
        self.data = [ord(c) % vocab_size for c in self.text]

    def __len__(self):
        return max(0, len(self.data) - self.seq_len)

    def __getitem__(self, idx):
        if not torch:
            return []
        chunk = self.data[idx : idx + self.seq_len]
        return torch.tensor(chunk, dtype=torch.long)


class BinaryTokenDataset(Dataset):
    """Loads pre-tokenized data from a binary file (uint16 format)."""

    def __init__(self, file_path, seq_len={{ blueprint.max_seq_len or 128 }}):
        self.seq_len = seq_len
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"Data file {file_path} not found")
        # Memory-map for efficiency with large files
        self.data = np.memmap(file_path, dtype=np.uint16, mode="r")
        print(f"Loaded {len(self.data):,} tokens from {file_path}")

    def __len__(self):
        return max(0, len(self.data) - self.seq_len)

    def __getitem__(self, idx):
        if not torch:
            return []
        chunk = self.data[idx : idx + self.seq_len].astype(np.int64)
        return torch.from_numpy(chunk)


class LimitedDataset(Dataset):
    """Wrap a dataset and cap its length."""

    def __init__(self, dataset: Dataset, max_length: int):
        self.dataset = dataset
        self.max_length = max(0, min(len(dataset), max_length))

    def __len__(self):
        return self.max_length

    def __getitem__(self, idx):
        return self.dataset[idx]


def _sample_features(
    seq_len: int = SEQ_LEN,
    hidden_size: int = HIDDEN_SIZE,
    vocab_size: int = VOCAB_SIZE,
    generator: torch.Generator | None = None,
) -> torch.Tensor:
    if generator is None:
        generator = _seeded_generator()
    if vocab_size:
        return torch.randint(0, vocab_size, (seq_len,), dtype=torch.long, generator=generator)
    return torch.randn(seq_len, hidden_size, generator=generator)


class SyntheticClassificationDataset(Dataset):
    def __init__(
        self,
        size: int = 256,
        seq_len: int = SEQ_LEN,
        hidden_size: int = HIDDEN_SIZE,
        num_classes: int = NUM_CLASSES,
        vocab_size: int = VOCAB_SIZE,
    ):
        self.size = size
        self.seq_len = seq_len
        self.hidden_size = hidden_size
        self.num_classes = max(2, num_classes)
        self.vocab_size = vocab_size

    def __len__(self):
        return self.size

    def __getitem__(self, idx):
        if not torch:
            return []
        generator = _seeded_generator(idx)
        x = _sample_features(self.seq_len, self.hidden_size, self.vocab_size, generator=generator)
        y = torch.randint(0, self.num_classes, (), dtype=torch.long, generator=generator)
        return x, y


class SyntheticRegressionDataset(Dataset):
    def __init__(
        self,
        size: int = 256,
        seq_len: int = SEQ_LEN,
        hidden_size: int = HIDDEN_SIZE,
        output_dim: int = NUM_OUTPUTS,
        vocab_size: int = VOCAB_SIZE,
    ):
        self.size = size
        self.seq_len = seq_len
        self.hidden_size = hidden_size
        self.output_dim = max(1, output_dim)
        self.vocab_size = vocab_size

    def __len__(self):
        return self.size

    def __getitem__(self, idx):
        if not torch:
            return []
        generator = _seeded_generator(idx)
        x = _sample_features(self.seq_len, self.hidden_size, self.vocab_size, generator=generator)
        y = torch.randn(self.output_dim, generator=generator)
        return x, y


class SyntheticReconstructionDataset(Dataset):
    def __init__(
        self,
        size: int = 256,
        seq_len: int = SEQ_LEN,
        hidden_size: int = HIDDEN_SIZE,
        vocab_size: int = VOCAB_SIZE,
    ):
        self.size = size
        self.seq_len = seq_len
        self.hidden_size = hidden_size
        self.vocab_size = vocab_size

    def __len__(self):
        return self.size

    def __getitem__(self, idx):
        if not torch:
            return []
        generator = _seeded_generator(idx)
        x = _sample_features(self.seq_len, self.hidden_size, self.vocab_size, generator=generator)
        y = x.clone() if isinstance(x, torch.Tensor) else x
        return x, y


class SyntheticTimeSeriesDataset(Dataset):
    def __init__(
        self,
        size: int = 256,
        lookback: int = LOOKBACK,
        hidden_size: int = HIDDEN_SIZE,
        horizon: int = HORIZON,
        output_dim: int = NUM_OUTPUTS,
    ):
        self.size = size
        self.lookback = lookback
        self.hidden_size = hidden_size
        self.horizon = max(1, horizon)
        self.output_dim = max(1, output_dim)

    def __len__(self):
        return self.size

    def __getitem__(self, idx):
        if not torch:
            return []
        generator = _seeded_generator(idx)
        x = torch.randn(self.lookback, self.hidden_size, generator=generator)
        y = torch.randn(self.horizon, self.output_dim, generator=generator)
        return x, y


class SyntheticDetectionDataset(Dataset):
    def __init__(
        self,
        size: int = 128,
        seq_len: int = SEQ_LEN,
        hidden_size: int = HIDDEN_SIZE,
        num_classes: int = NUM_CLASSES,
        num_anchors: int = NUM_ANCHORS,
        vocab_size: int = VOCAB_SIZE,
    ):
        self.size = size
        self.seq_len = seq_len
        self.hidden_size = hidden_size
        self.num_classes = max(2, num_classes)
        self.num_anchors = max(1, num_anchors)
        self.vocab_size = vocab_size

    def __len__(self):
        return self.size

    def __getitem__(self, idx):
        if not torch:
            return []
        generator = _seeded_generator(idx)
        x = _sample_features(self.seq_len, self.hidden_size, self.vocab_size, generator=generator)
        labels = torch.randint(
            0,
            self.num_classes,
            (self.num_anchors,),
            dtype=torch.long,
            generator=generator,
        )
        boxes = torch.rand(self.num_anchors, 4, generator=generator)
        return x, {"labels": labels, "boxes": boxes}


class SyntheticSegmentationDataset(Dataset):
    def __init__(
        self,
        size: int = 128,
        seq_len: int = SEQ_LEN,
        hidden_size: int = HIDDEN_SIZE,
        num_classes: int = NUM_CLASSES,
        mask_resolution: int = MASK_RESOLUTION,
        vocab_size: int = VOCAB_SIZE,
    ):
        self.size = size
        self.seq_len = seq_len
        self.hidden_size = hidden_size
        self.num_classes = max(2, num_classes)
        self.mask_resolution = max(8, mask_resolution)
        self.vocab_size = vocab_size

    def __len__(self):
        return self.size

    def __getitem__(self, idx):
        if not torch:
            return []
        generator = _seeded_generator(idx)
        x = _sample_features(self.seq_len, self.hidden_size, self.vocab_size, generator=generator)
        mask = torch.randint(
            0,
            self.num_classes,
            (self.mask_resolution, self.mask_resolution),
            dtype=torch.long,
            generator=generator,
        )
        return x, mask


class SyntheticGraphDataset(Dataset):
    def __init__(
        self,
        size: int = 256,
        num_nodes: int = NUM_NODES,
        hidden_size: int = HIDDEN_SIZE,
        num_classes: int = NUM_CLASSES,
        output_dim: int = NUM_OUTPUTS,
        mode: str = "graph_classification",
    ):
        self.size = size
        self.num_nodes = max(4, num_nodes)
        self.hidden_size = hidden_size
        self.num_classes = max(2, num_classes)
        self.output_dim = max(1, output_dim)
        self.mode = mode

    def __len__(self):
        return self.size

    def __getitem__(self, idx):
        if not torch:
            return []
        generator = _seeded_generator(idx)
        x = torch.randn(self.num_nodes, self.hidden_size, generator=generator)
        if self.mode == "node_classification":
            y = torch.randint(
                0,
                self.num_classes,
                (self.num_nodes,),
                dtype=torch.long,
                generator=generator,
            )
        elif self.mode == "link_prediction":
            y = torch.randint(
                0,
                2,
                (self.num_nodes, self.num_nodes),
                dtype=torch.float32,
                generator=generator,
            )
        elif self.mode == "recommendation":
            y = torch.randn(self.output_dim, generator=generator)
        else:
            y = torch.randint(0, self.num_classes, (), dtype=torch.long, generator=generator)
        return x, y


class SyntheticPairDataset(Dataset):
    def __init__(
        self,
        size: int = 256,
        seq_len: int = SEQ_LEN,
        hidden_size: int = HIDDEN_SIZE,
        vocab_size: int = VOCAB_SIZE,
    ):
        self.size = size
        self.seq_len = seq_len
        self.hidden_size = hidden_size
        self.vocab_size = vocab_size

    def __len__(self):
        return self.size

    def __getitem__(self, idx):
        if not torch:
            return []
        generator = _seeded_generator(idx)
        left = _sample_features(self.seq_len, self.hidden_size, self.vocab_size, generator=generator)
        right = _sample_features(self.seq_len, self.hidden_size, self.vocab_size, generator=generator)
        return left, right


class SyntheticRLDataset(Dataset):
    def __init__(
        self,
        size: int = 256,
        seq_len: int = SEQ_LEN,
        hidden_size: int = HIDDEN_SIZE,
        action_space: str = ACTION_SPACE,
        num_actions: int = NUM_ACTIONS,
        action_dim: int = ACTION_DIM,
        mode: str = "policy_gradient",
        vocab_size: int = VOCAB_SIZE,
    ):
        self.size = size
        self.seq_len = seq_len
        self.hidden_size = hidden_size
        self.action_space = action_space
        self.num_actions = max(2, num_actions)
        self.action_dim = max(1, action_dim)
        self.mode = mode
        self.vocab_size = vocab_size

    def __len__(self):
        return self.size

    def _sample_action(self, generator: torch.Generator):
        if self.action_space == "continuous":
            return torch.randn(self.action_dim, generator=generator)
        return torch.randint(0, self.num_actions, (), dtype=torch.long, generator=generator)

    def __getitem__(self, idx):
        if not torch:
            return []
        generator = _seeded_generator(idx)
        x = _sample_features(self.seq_len, self.hidden_size, self.vocab_size, generator=generator)
        if self.mode == "value_based":
            target_dim = self.action_dim if self.action_space == "continuous" else self.num_actions
            y = torch.randn(target_dim, generator=generator)
        elif self.mode == "actor_critic":
            y = {
                "actions": self._sample_action(generator),
                "advantages": torch.randn((), generator=generator),
                "returns": torch.randn((), generator=generator),
            }
        elif self.mode == "model_based":
            y = {"next_state": torch.randn(self.hidden_size, generator=generator)}
        else:
            y = {
                "actions": self._sample_action(generator),
                "advantages": torch.randn((), generator=generator),
            }
        return x, y


def _auto_sample_name() -> str:
    if HEAD_TYPE in {
        "classification_head",
        "hierarchical_classification_head",
        "graph_classification_head",
        "node_classification_head",
        "link_prediction_head",
        "recommendation_head",
    }:
        return "synthetic_graph" if "graph" in HEAD_TYPE else "synthetic_classification"
    if HEAD_TYPE == "regression_head":
        return "synthetic_regression"
    if HEAD_TYPE == "time_series_head":
        return "synthetic_time_series"
    if HEAD_TYPE == "detection_head":
        return "synthetic_detection"
    if HEAD_TYPE == "segmentation_head":
        return "synthetic_segmentation"
    if HEAD_TYPE == "embedding_head":
        return "synthetic_pair"
    if HEAD_TYPE == "ranking_head":
        return "synthetic_pair"
    if HEAD_TYPE in {"rl_policy_head", "rl_value_head", "rl_actor_critic_head", "rl_model_head"}:
        return "synthetic_rl"
    if VOCAB_SIZE:
        return "synthetic_text"
    return "synthetic_reconstruction"


SAMPLE_DATASETS = {
    "synthetic_text": lambda seq_len: SyntheticTextDataset(size=256, seq_len=seq_len),
    "synthetic_classification": lambda seq_len: SyntheticClassificationDataset(
        size=256, seq_len=seq_len
    ),
    "synthetic_regression": lambda seq_len: SyntheticRegressionDataset(size=256, seq_len=seq_len),
    "synthetic_reconstruction": lambda seq_len: SyntheticReconstructionDataset(
        size=256, seq_len=seq_len
    ),
    "synthetic_time_series": lambda seq_len: SyntheticTimeSeriesDataset(size=256),
    "synthetic_detection": lambda seq_len: SyntheticDetectionDataset(size=128, seq_len=seq_len),
    "synthetic_segmentation": lambda seq_len: SyntheticSegmentationDataset(size=128, seq_len=seq_len),
    "synthetic_graph": lambda seq_len: SyntheticGraphDataset(
        size=256,
        mode=(
            "node_classification"
            if HEAD_TYPE == "node_classification_head"
            else "link_prediction"
            if HEAD_TYPE == "link_prediction_head"
            else "recommendation"
            if HEAD_TYPE == "recommendation_head"
            else "graph_classification"
        ),
    ),
    "synthetic_pair": lambda seq_len: SyntheticPairDataset(size=256, seq_len=seq_len),
    "synthetic_rl": lambda seq_len: SyntheticRLDataset(
        size=256,
        seq_len=seq_len,
        mode=(
            "value_based"
            if LOSS_TYPE == "td_loss"
            else "actor_critic"
            if LOSS_TYPE == "actor_critic_loss"
            else "model_based"
            if LOSS_TYPE == "model_based_loss"
            else "policy_gradient"
        ),
    ),
}

REMOTE_DATASETS = {
    "hf:wikitext-2": {
        "provider": "hf",
        "path": "wikitext",
        "name": "wikitext-2-raw-v1",
        "category": "text_lm",
        "description": "Wikitext-2 raw tokens (small).",
        "text_field": "text",
    },
    "hf:wikitext-103": {
        "provider": "hf",
        "path": "wikitext",
        "name": "wikitext-103-raw-v1",
        "category": "text_lm",
        "description": "Wikitext-103 raw tokens (medium).",
        "text_field": "text",
    },
    "hf:openwebtext": {
        "provider": "hf",
        "path": "openwebtext",
        "category": "text_lm",
        "description": "OpenWebText (large).",
        "text_field": "text",
    },
    "hf:c4": {
        "provider": "hf",
        "path": "c4",
        "name": "en",
        "category": "text_lm",
        "description": "Colossal Clean Crawled Corpus (very large).",
        "text_field": "text",
    },
    "hf:ag_news": {
        "provider": "hf",
        "path": "ag_news",
        "category": "text_classification",
        "description": "AG News topic classification.",
        "text_field": "text",
        "label_field": "label",
    },
    "hf:imdb": {
        "provider": "hf",
        "path": "imdb",
        "category": "text_classification",
        "description": "IMDb movie reviews sentiment.",
        "text_field": "text",
        "label_field": "label",
    },
    "hf:yelp_polarity": {
        "provider": "hf",
        "path": "yelp_polarity",
        "category": "text_classification",
        "description": "Yelp polarity reviews.",
        "text_field": "text",
        "label_field": "label",
    },
    "hf:dbpedia_14": {
        "provider": "hf",
        "path": "dbpedia_14",
        "category": "text_classification",
        "description": "DBPedia 14-class ontology.",
        "label_field": "label",
    },
    "hf:glue/qqp": {
        "provider": "hf",
        "path": "glue",
        "name": "qqp",
        "category": "text_pair",
        "description": "Quora question pairs.",
        "pair_fields": ("question1", "question2"),
        "label_field": "label",
    },
    "hf:glue/mrpc": {
        "provider": "hf",
        "path": "glue",
        "name": "mrpc",
        "category": "text_pair",
        "description": "MRPC paraphrase corpus.",
        "pair_fields": ("sentence1", "sentence2"),
        "label_field": "label",
    },
    "hf:glue/stsb": {
        "provider": "hf",
        "path": "glue",
        "name": "stsb",
        "category": "text_regression",
        "description": "Semantic textual similarity benchmark.",
        "pair_fields": ("sentence1", "sentence2"),
        "label_field": "label",
    },
    "hf:snli": {
        "provider": "hf",
        "path": "snli",
        "category": "text_pair",
        "description": "SNLI entailment pairs.",
        "pair_fields": ("premise", "hypothesis"),
        "label_field": "label",
    },
    "hf:ms_marco": {
        "provider": "hf",
        "path": "ms_marco",
        "name": "passage",
        "category": "text_pair",
        "description": "MS MARCO passage ranking pairs.",
        "pair_fields": ("query", "passage"),
        "label_field": "label",
    },
    "hf:natural_questions": {
        "provider": "hf",
        "path": "natural_questions",
        "category": "text_pair",
        "description": "Natural Questions query-document pairs.",
        "pair_fields": ("question", "document"),
        "label_field": "label",
    },
    "hf:mnist": {
        "provider": "hf",
        "path": "mnist",
        "category": "image_classification",
        "description": "MNIST handwritten digits.",
    },
    "hf:fashion_mnist": {
        "provider": "hf",
        "path": "fashion_mnist",
        "category": "image_classification",
        "description": "Fashion-MNIST apparel classification.",
    },
    "hf:cifar10": {
        "provider": "hf",
        "path": "cifar10",
        "category": "image_classification",
        "description": "CIFAR-10 images.",
    },
    "hf:food101": {
        "provider": "hf",
        "path": "food101",
        "category": "image_classification",
        "description": "Food-101 images.",
    },
    "hf:cppe-5": {
        "provider": "hf",
        "path": "cppe-5",
        "category": "detection",
        "description": "PPE object detection (small).",
        "image_field": "image",
        "objects_field": "objects",
    },
    "hf:coco": {
        "provider": "hf",
        "path": "coco",
        "name": "2017",
        "category": "detection",
        "description": "COCO 2017 detection/segmentation (large).",
        "image_field": "image",
        "objects_field": "objects",
    },
    "hf:oxford_iiit_pet": {
        "provider": "hf",
        "path": "oxford_iiit_pet",
        "category": "segmentation",
        "description": "Oxford-IIIT Pet segmentation.",
        "image_field": "image",
        "mask_field": "mask",
    },
    "hf:ade20k": {
        "provider": "hf",
        "path": "ade20k",
        "category": "segmentation",
        "description": "ADE20K scene parsing (large).",
        "image_field": "image",
        "mask_field": "annotation",
    },
    "hf:monash_tsf/traffic": {
        "provider": "hf",
        "path": "monash_tsf",
        "name": "traffic",
        "category": "time_series",
        "description": "Monash TSF traffic series.",
        "series_field": "target",
    },
    "hf:monash_tsf/electricity": {
        "provider": "hf",
        "path": "monash_tsf",
        "name": "electricity",
        "category": "time_series",
        "description": "Monash TSF electricity series.",
        "series_field": "target",
    },
    "hf:monash_tsf/m4_hourly": {
        "provider": "hf",
        "path": "monash_tsf",
        "name": "m4_hourly",
        "category": "time_series",
        "description": "M4 hourly time series (large).",
        "series_field": "target",
    },
    "hf:adult": {
        "provider": "hf",
        "path": "mstz/adult",
        "category": "tabular_classification",
        "description": "Adult census income classification.",
    },
    "hf:bank": {
        "provider": "hf",
        "path": "mstz/bank",
        "category": "tabular_classification",
        "description": "Bank marketing classification.",
    },
    "hf:credit_card_default": {
        "provider": "hf",
        "path": "imodels/credit-card",
        "category": "tabular_classification",
        "description": "Credit card default prediction.",
    },
    "hf:california_housing": {
        "provider": "hf",
        "path": "gvlassis/california_housing",
        "category": "tabular_regression",
        "description": "California housing regression.",
    },
    "hf:diabetes": {
        "provider": "hf",
        "path": "korelidw/diabetes.data",
        "category": "tabular_regression",
        "description": "Diabetes progression regression.",
    },
    "hf:speech_commands": {
        "provider": "hf",
        "path": "speech_commands",
        "category": "audio_classification",
        "description": "Speech Commands keywords.",
        "audio_field": "audio",
        "label_field": "label",
    },
    "hf:esc50": {
        "provider": "hf",
        "path": "esc50",
        "category": "audio_classification",
        "description": "ESC-50 environmental sounds.",
        "audio_field": "audio",
        "label_field": "label",
    },
    "hf:common_voice": {
        "provider": "hf",
        "path": "common_voice",
        "name": "en",
        "category": "audio_classification",
        "description": "Common Voice English (large).",
        "audio_field": "audio",
        "text_field": "sentence",
    },
    "hf:librispeech_asr": {
        "provider": "hf",
        "path": "librispeech_asr",
        "name": "clean",
        "category": "audio_classification",
        "description": "LibriSpeech ASR (large).",
        "audio_field": "audio",
        "text_field": "text",
    },
    "hf:ucf101": {
        "provider": "hf",
        "path": "ucf101",
        "category": "video_classification",
        "description": "UCF101 action recognition.",
    },
    "hf:kinetics400": {
        "provider": "hf",
        "path": "kinetics400",
        "category": "video_classification",
        "description": "Kinetics-400 actions (large).",
    },
    "hf:something_something_v2": {
        "provider": "hf",
        "path": "something_something_v2",
        "category": "video_classification",
        "description": "Something-Something V2 (large).",
    },
    "hf:coco_captions": {
        "provider": "hf",
        "path": "coco_captions",
        "category": "multimodal",
        "description": "COCO image-caption pairs.",
        "image_field": "image",
        "text_field": "caption",
    },
    "hf:flickr30k": {
        "provider": "hf",
        "path": "flickr30k",
        "category": "multimodal",
        "description": "Flickr30k image-caption pairs.",
        "image_field": "image",
        "text_field": "caption",
    },
    "hf:conceptual_captions": {
        "provider": "hf",
        "path": "conceptual_captions",
        "category": "multimodal",
        "description": "Conceptual Captions (large).",
        "image_field": "image",
        "text_field": "caption",
    },
    "hf:ogbn_arxiv": {
        "provider": "hf",
        "path": "ogbn_arxiv",
        "category": "graph",
        "description": "OGBN-Arxiv citation graph.",
    },
    "hf:ogbn_products": {
        "provider": "hf",
        "path": "ogbn_products",
        "category": "graph",
        "description": "OGBN-Products graph.",
    },
}

DATASET_GROUPS = {
    "language_modeling": [
        "hf:wikitext-2",
        "hf:wikitext-103",
        "hf:openwebtext",
        "hf:c4",
    ],
    "text_classification": [
        "hf:ag_news",
        "hf:imdb",
        "hf:yelp_polarity",
        "hf:dbpedia_14",
    ],
    "text_pair": [
        "hf:glue/qqp",
        "hf:glue/mrpc",
        "hf:snli",
        "hf:ms_marco",
        "hf:natural_questions",
    ],
    "text_regression": ["hf:glue/stsb"],
    "image_classification": [
        "hf:mnist",
        "hf:fashion_mnist",
        "hf:cifar10",
        "hf:food101",
    ],
    "detection": ["hf:cppe-5", "hf:coco"],
    "segmentation": ["hf:oxford_iiit_pet", "hf:ade20k", "hf:coco"],
    "time_series": [
        "hf:monash_tsf/traffic",
        "hf:monash_tsf/electricity",
        "hf:monash_tsf/m4_hourly",
    ],
    "tabular_classification": ["hf:adult", "hf:bank", "hf:credit_card_default"],
    "tabular_regression": ["hf:california_housing", "hf:diabetes"],
    "audio_classification": [
        "hf:speech_commands",
        "hf:esc50",
        "hf:common_voice",
        "hf:librispeech_asr",
    ],
    "video_classification": [
        "hf:ucf101",
        "hf:kinetics400",
        "hf:something_something_v2",
    ],
    "multimodal_contrastive": [
        "hf:coco_captions",
        "hf:flickr30k",
        "hf:conceptual_captions",
    ],
    "graph": ["hf:ogbn_arxiv", "hf:ogbn_products"],
}

PAIR_HEADS = {"embedding_head", "ranking_head"}
GRAPH_HEADS = {
    "graph_classification_head",
    "node_classification_head",
    "link_prediction_head",
    "recommendation_head",
}
RL_HEADS = {"rl_policy_head", "rl_value_head", "rl_actor_critic_head", "rl_model_head"}
CLASSIFICATION_HEADS = {
    "classification_head",
    "hierarchical_classification_head",
    "graph_classification_head",
    "node_classification_head",
}

TEXT_FIELDS = (
    "text",
    "sentence",
    "sentence1",
    "sentence2",
    "premise",
    "hypothesis",
    "question",
    "question1",
    "question2",
    "query",
    "document",
    "passage",
    "review",
    "content",
    "article",
    "abstract",
    "title",
    "body",
    "caption",
)
IMAGE_FIELDS = ("image", "img", "pixel_values", "pixels")
AUDIO_FIELDS = ("audio", "speech")
VIDEO_FIELDS = ("video", "frames", "clip", "clips")
LABEL_FIELDS = ("label", "labels", "target", "targets", "category", "class", "label_id")
MASK_FIELDS = ("mask", "masks", "segmentation", "segmentation_mask")
BOX_FIELDS = ("bbox", "bboxes", "boxes", "bounding_boxes", "box")


def _resolve_task_mode() -> str:
    if LOSS_TYPE in {"contrastive_loss", "pairwise_ranking_loss"}:
        return "pair"
    if HEAD_TYPE in PAIR_HEADS:
        return "pair"
    if HEAD_TYPE == "detection_head":
        return "detection"
    if HEAD_TYPE == "segmentation_head":
        return "segmentation"
    if HEAD_TYPE == "time_series_head":
        return "time_series"
    if HEAD_TYPE in RL_HEADS:
        return "rl"
    if HEAD_TYPE in GRAPH_HEADS:
        return "graph"
    if HEAD_TYPE == "regression_head":
        return "regression"
    if HEAD_TYPE in CLASSIFICATION_HEADS:
        return "classification"
    if VOCAB_SIZE:
        return "language_modeling"
    return "reconstruction"


TASK_MODE = _resolve_task_mode()


def available_remote_datasets() -> list[str]:
    return ["auto", *sorted(REMOTE_DATASETS.keys())]


def list_remote_datasets() -> list[tuple[str, str]]:
    entries: list[tuple[str, str]] = []
    for name in sorted(REMOTE_DATASETS):
        spec = REMOTE_DATASETS[name]
        category = spec.get("category")
        desc = spec.get("description", "")
        label = f"{category}: {desc}".strip(": ") if category else desc
        entries.append((name, label))
    return entries


def _auto_remote_group() -> str | None:
    if TASK_MODE == "pair":
        return "multimodal_contrastive" if "image" in INPUT_MODALITIES else "text_pair"
    if TASK_MODE == "detection":
        return "detection"
    if TASK_MODE == "segmentation":
        return "segmentation"
    if TASK_MODE == "time_series":
        return "time_series"
    if TASK_MODE == "graph":
        return "graph"
    if TASK_MODE == "rl":
        if "image" in INPUT_MODALITIES:
            return "image_classification"
        if "tabular" in INPUT_MODALITIES:
            return "tabular_classification"
        if "text" in INPUT_MODALITIES:
            return "text_classification"
        return "text_classification"
    if "video" in INPUT_MODALITIES:
        return "video_classification"
    if "audio" in INPUT_MODALITIES:
        return "audio_classification"
    if "image" in INPUT_MODALITIES:
        return "image_classification"
    if "tabular" in INPUT_MODALITIES:
        return "tabular_regression" if TASK_MODE == "regression" else "tabular_classification"
    if "time_series" in INPUT_MODALITIES:
        return "time_series"
    if TASK_MODE == "regression":
        return "text_regression"
    if TASK_MODE == "classification":
        return "text_classification"
    if TASK_MODE == "language_modeling":
        return "language_modeling"
    return "text_classification"


def suggested_remote_datasets() -> list[str]:
    group = _auto_remote_group()
    if not group:
        return []
    return DATASET_GROUPS.get(group, [])


def _resolve_remote_dataset_spec(dataset_name: str, dataset_config: str | None) -> dict:
    if dataset_name == "auto":
        suggestions = suggested_remote_datasets()
        if not suggestions:
            raise ValueError("No suggested datasets for this task. Use --sample-data instead.")
        dataset_name = suggestions[0]
    if dataset_name in REMOTE_DATASETS:
        spec = dict(REMOTE_DATASETS[dataset_name])
    else:
        if not dataset_name.startswith("hf:"):
            raise ValueError(
                f"Unknown dataset '{dataset_name}'. Use --list-datasets to see curated names."
            )
        spec = {
            "provider": "hf",
            "path": dataset_name.replace("hf:", "", 1),
            "name": None,
            "category": "custom",
            "description": "Custom Hugging Face dataset.",
        }
    if dataset_config:
        spec["name"] = dataset_config
    return spec


def _resolve_hf_split(split: str | None, dataset_size: int | None) -> str:
    split = split or "train"
    if dataset_size is None or dataset_size <= 0:
        return split
    return f"{split}[:{dataset_size}]"


def _load_hf_dataset(
    spec: dict,
    split: str | None,
    dataset_size: int | None,
    cache_dir: str | None,
):
    if load_dataset is None:  # pragma: no cover
        raise RuntimeError(
            "Hugging Face datasets not installed. Install with `pip install datasets`."
        )
    path = spec["path"]
    name = spec.get("name")
    split_spec = _resolve_hf_split(split, dataset_size)
    if name:
        return load_dataset(path, name, split=split_spec, cache_dir=cache_dir)
    return load_dataset(path, split=split_spec, cache_dir=cache_dir)


def _flatten_numeric(value) -> np.ndarray | None:
    if value is None:
        return None
    if isinstance(value, (int, float, np.number)):
        return np.asarray([value], dtype=np.float32)
    if isinstance(value, str):
        return None
    if isinstance(value, dict):
        if "array" in value:
            return _flatten_numeric(value["array"])
        if "data" in value:
            return _flatten_numeric(value["data"])
        numeric = [float(v) for v in value.values() if isinstance(v, (int, float, np.number))]
        if numeric:
            return np.asarray(numeric, dtype=np.float32)
        return None
    try:
        arr = np.asarray(value)
    except Exception:
        return None
    if arr.size == 0:
        return None
    if np.issubdtype(arr.dtype, np.number):
        return arr.astype(np.float32).reshape(-1)
    return None


def _pad_or_truncate(array: np.ndarray, size: int, pad_value: float = 0.0) -> np.ndarray:
    if array.size < size:
        pad = np.full(size - array.size, pad_value, dtype=array.dtype)
        return np.concatenate([array, pad])
    return array[:size]


def _to_token_sequence(
    value,
    seq_len: int,
    vocab_size: int,
    generator: torch.Generator | None = None,
) -> torch.Tensor:
    if generator is None:
        generator = _seeded_generator()
    if value is None:
        return torch.randint(0, vocab_size, (seq_len,), generator=generator)
    if isinstance(value, str):
        ids = [ord(c) % vocab_size for c in value]
    else:
        flat = _flatten_numeric(value)
        if flat is None:
            return torch.randint(0, vocab_size, (seq_len,), generator=generator)
        ids = np.abs(flat).astype(np.int64) % max(2, vocab_size)
    ids = _pad_or_truncate(np.asarray(ids, dtype=np.int64), seq_len, pad_value=0)
    return torch.tensor(ids, dtype=torch.long)


def _to_feature_sequence(
    value,
    seq_len: int,
    hidden_size: int,
    generator: torch.Generator | None = None,
) -> torch.Tensor:
    if generator is None:
        generator = _seeded_generator()
    flat = _flatten_numeric(value)
    if flat is None:
        return torch.randn(seq_len, hidden_size, generator=generator)
    flat = _pad_or_truncate(flat, seq_len * hidden_size, pad_value=0.0)
    return torch.tensor(flat, dtype=torch.float32).view(seq_len, hidden_size)


def _encode_input(
    value,
    seq_len: int,
    generator: torch.Generator | None = None,
) -> torch.Tensor:
    if VOCAB_SIZE:
        return _to_token_sequence(value, seq_len, max(2, VOCAB_SIZE), generator=generator)
    return _to_feature_sequence(value, seq_len, HIDDEN_SIZE, generator=generator)


def _extract_text(sample: dict, preferred: str | None = None):
    if preferred and preferred in sample:
        return sample[preferred]
    for key in TEXT_FIELDS:
        if key in sample:
            return sample[key]
    return None


def _extract_text_pair(sample: dict, preferred: tuple[str, str] | None):
    if preferred and preferred[0] in sample and preferred[1] in sample:
        return sample[preferred[0]], sample[preferred[1]]
    keys = [key for key in TEXT_FIELDS if key in sample]
    if len(keys) >= 2:
        return sample[keys[0]], sample[keys[1]]
    return None


def _extract_image(sample: dict, preferred: str | None = None):
    if preferred and preferred in sample:
        return sample[preferred]
    for key in IMAGE_FIELDS:
        if key in sample:
            return sample[key]
    return None


def _extract_audio(sample: dict, preferred: str | None = None):
    if preferred and preferred in sample:
        return sample[preferred]
    for key in AUDIO_FIELDS:
        if key in sample:
            return sample[key]
    return None


def _extract_video(sample: dict, preferred: str | None = None):
    if preferred and preferred in sample:
        return sample[preferred]
    for key in VIDEO_FIELDS:
        if key in sample:
            return sample[key]
    return None


def _extract_label(sample: dict, preferred: str | None = None):
    if preferred and preferred in sample:
        return sample[preferred]
    for key in LABEL_FIELDS:
        if key in sample:
            return sample[key]
    return None


def _extract_objects(sample: dict, preferred: str | None = None):
    if preferred and preferred in sample:
        return sample[preferred]
    if "objects" in sample:
        return sample["objects"]
    return None


def _extract_mask(sample: dict, preferred: str | None = None):
    if preferred and preferred in sample:
        return sample[preferred]
    for key in MASK_FIELDS:
        if key in sample:
            return sample[key]
    objects = sample.get("objects")
    if isinstance(objects, dict):
        for key in MASK_FIELDS:
            if key in objects:
                return objects[key]
    return None


def _extract_boxes_and_labels(sample: dict, spec: dict):
    boxes = None
    labels = None
    for key in BOX_FIELDS:
        if key in sample:
            boxes = sample[key]
            break
    label_field = spec.get("label_field")
    if label_field and label_field in sample:
        labels = sample[label_field]
    objects = _extract_objects(sample, spec.get("objects_field"))
    if isinstance(objects, dict):
        for key in BOX_FIELDS:
            if key in objects:
                boxes = objects[key]
                break
        for key in LABEL_FIELDS:
            if key in objects:
                labels = objects[key]
                break
    elif isinstance(objects, list):
        box_list = []
        label_list = []
        for item in objects:
            if not isinstance(item, dict):
                continue
            for key in BOX_FIELDS:
                if key in item:
                    box_list.append(item[key])
                    break
            for key in LABEL_FIELDS:
                if key in item:
                    label_list.append(item[key])
                    break
        if box_list:
            boxes = box_list
        if label_list:
            labels = label_list
    return boxes, labels


def _extract_numeric_features(
    sample: dict, skip_fields: set[str] | None = None
) -> np.ndarray | None:
    values: list[np.ndarray] = []
    for key, value in sample.items():
        if skip_fields and key in skip_fields:
            continue
        flat = _flatten_numeric(value)
        if flat is not None:
            values.append(flat)
    if not values:
        return None
    return np.concatenate(values)


def _coerce_label(
    value,
    num_classes: int,
    generator: torch.Generator | None = None,
) -> torch.Tensor:
    if generator is None:
        generator = _seeded_generator()
    if value is None:
        return torch.randint(0, max(2, num_classes), (), dtype=torch.long, generator=generator)
    if isinstance(value, (list, tuple, np.ndarray)):
        if len(value) == 0:
            return torch.randint(
                0,
                max(2, num_classes),
                (),
                dtype=torch.long,
                generator=generator,
            )
        value = value[0]
    if isinstance(value, bool):
        idx = int(value)
    elif isinstance(value, (int, np.integer)):
        idx = int(value)
    elif isinstance(value, float):
        idx = int(value)
    elif isinstance(value, str):
        idx = sum(ord(c) for c in value)
    else:
        idx = 0
    return torch.tensor(idx % max(2, num_classes), dtype=torch.long)


def _coerce_regression(
    value,
    output_dim: int,
    generator: torch.Generator | None = None,
) -> torch.Tensor:
    if generator is None:
        generator = _seeded_generator()
    flat = _flatten_numeric(value)
    if flat is None or flat.size == 0:
        return torch.randn(output_dim, generator=generator)
    flat = _pad_or_truncate(flat, output_dim, pad_value=0.0)
    return torch.tensor(flat, dtype=torch.float32)


def _normalize_boxes(boxes) -> np.ndarray:
    arr = np.asarray(boxes, dtype=np.float32)
    if arr.ndim == 1:
        arr = arr.reshape(1, -1)
    if arr.shape[1] != 4:
        arr = _pad_or_truncate(arr.reshape(-1), 4).reshape(1, 4)
    max_val = float(np.max(arr)) if arr.size else 1.0
    if max_val > 1.0:
        arr = arr / max_val
    return np.clip(arr, 0.0, 1.0)


def _pad_boxes(arr: np.ndarray, target: int) -> np.ndarray:
    if arr.shape[0] >= target:
        return arr[:target]
    pad = np.zeros((target - arr.shape[0], 4), dtype=np.float32)
    return np.concatenate([arr, pad])


def _pad_labels(arr: np.ndarray, target: int) -> np.ndarray:
    if arr.shape[0] >= target:
        return arr[:target]
    pad = np.zeros((target - arr.shape[0],), dtype=np.int64)
    return np.concatenate([arr, pad])


def _build_detection_target(
    sample: dict,
    spec: dict,
    generator: torch.Generator | None = None,
) -> dict[str, torch.Tensor]:
    if generator is None:
        generator = _seeded_generator()
    boxes, labels = _extract_boxes_and_labels(sample, spec)
    if boxes is None:
        boxes = torch.rand(NUM_ANCHORS, 4, generator=generator).numpy().astype(np.float32)
    boxes = _normalize_boxes(boxes)
    boxes = _pad_boxes(boxes, NUM_ANCHORS)
    if labels is None:
        label_arr = (
            torch.randint(
                0,
                max(2, NUM_CLASSES),
                (boxes.shape[0],),
                generator=generator,
                dtype=torch.long,
            )
            .numpy()
            .astype(np.int64)
        )
    else:
        if not isinstance(labels, (list, tuple, np.ndarray)):
            labels = [labels]
        label_arr = np.asarray(
            [_coerce_label(v, NUM_CLASSES, generator=generator).item() for v in labels],
            dtype=np.int64,
        )
    label_arr = _pad_labels(label_arr, NUM_ANCHORS)
    return {
        "labels": torch.tensor(label_arr, dtype=torch.long),
        "boxes": torch.tensor(boxes, dtype=torch.float32),
    }


def _build_segmentation_target(
    sample: dict,
    spec: dict,
    generator: torch.Generator | None = None,
) -> torch.Tensor:
    if generator is None:
        generator = _seeded_generator()
    mask = _extract_mask(sample, spec.get("mask_field"))
    if mask is None:
        return torch.randint(
            0,
            max(2, NUM_CLASSES),
            (MASK_RESOLUTION, MASK_RESOLUTION),
            generator=generator,
            dtype=torch.long,
        )
    else:
        mask_arr = np.asarray(mask)
        if mask_arr.ndim == 3:
            mask_arr = mask_arr[..., 0]
        mask_arr = np.resize(mask_arr, (MASK_RESOLUTION, MASK_RESOLUTION))
        mask_arr = mask_arr.astype(np.int64) % max(2, NUM_CLASSES)
    return torch.tensor(mask_arr, dtype=torch.long)


def _build_time_series_pair(value, generator: torch.Generator | None = None):
    if generator is None:
        generator = _seeded_generator()
    flat = _flatten_numeric(value)
    if flat is None or flat.size == 0:
        return (
            torch.randn(LOOKBACK, HIDDEN_SIZE, generator=generator),
            torch.randn(HORIZON, NUM_OUTPUTS, generator=generator),
        )
    needed = LOOKBACK * HIDDEN_SIZE + HORIZON * NUM_OUTPUTS
    flat = _pad_or_truncate(flat, needed, pad_value=0.0)
    x_flat = flat[: LOOKBACK * HIDDEN_SIZE]
    y_flat = flat[LOOKBACK * HIDDEN_SIZE : LOOKBACK * HIDDEN_SIZE + HORIZON * NUM_OUTPUTS]
    x = torch.tensor(x_flat, dtype=torch.float32).view(LOOKBACK, HIDDEN_SIZE)
    y = torch.tensor(y_flat, dtype=torch.float32).view(HORIZON, NUM_OUTPUTS)
    return x, y


def _coerce_action(
    value,
    generator: torch.Generator | None = None,
) -> torch.Tensor:
    if generator is None:
        generator = _seeded_generator()
    if ACTION_SPACE == "continuous":
        flat = _flatten_numeric(value)
        if flat is None:
            return torch.randn(ACTION_DIM, generator=generator)
        flat = _pad_or_truncate(flat, ACTION_DIM, pad_value=0.0)
        return torch.tensor(flat, dtype=torch.float32)
    if value is None:
        return torch.randint(
            0,
            max(2, NUM_ACTIONS),
            (),
            dtype=torch.long,
            generator=generator,
        )
    if isinstance(value, (list, tuple, np.ndarray)):
        value = value[0] if len(value) else 0
    return torch.tensor(int(value) % max(2, NUM_ACTIONS), dtype=torch.long)


def _coerce_scalar(
    value,
    generator: torch.Generator | None = None,
) -> torch.Tensor:
    if generator is None:
        generator = _seeded_generator()
    if value is None:
        return torch.randn((), generator=generator)
    if isinstance(value, (list, tuple, np.ndarray)):
        value = value[0] if len(value) else 0.0
    try:
        return torch.tensor(float(value))
    except Exception:
        return torch.randn((), generator=generator)


def _extract_rl_field(sample: dict, keys: tuple[str, ...]):
    for key in keys:
        if key in sample:
            return sample[key]
    return None


def _build_rl_target(
    sample: dict,
    generator: torch.Generator | None = None,
) -> dict | torch.Tensor:
    if generator is None:
        generator = _seeded_generator()
    action_val = _extract_rl_field(sample, ("action", "actions"))
    reward_val = _extract_rl_field(sample, ("reward", "returns", "return"))
    next_state_val = _extract_rl_field(sample, ("next_state", "next_observation", "next_obs"))
    if LOSS_TYPE == "td_loss":
        return {"targets": _coerce_scalar(reward_val, generator=generator)}
    if LOSS_TYPE == "model_based_loss":
        if next_state_val is None:
            return {"next_state": torch.randn(HIDDEN_SIZE, generator=generator)}
        flat = _flatten_numeric(next_state_val)
        if flat is None:
            return {"next_state": torch.randn(HIDDEN_SIZE, generator=generator)}
        flat = _pad_or_truncate(flat, HIDDEN_SIZE, pad_value=0.0)
        return {"next_state": torch.tensor(flat, dtype=torch.float32)}
    action = _coerce_action(action_val, generator=generator)
    if LOSS_TYPE == "actor_critic_loss":
        return {
            "actions": action,
            "advantages": _coerce_scalar(reward_val, generator=generator),
            "returns": _coerce_scalar(reward_val, generator=generator),
        }
    if LOSS_TYPE == "policy_gradient_loss":
        return {
            "actions": action,
            "advantages": _coerce_scalar(reward_val, generator=generator),
        }
    return {"actions": action, "advantages": _coerce_scalar(reward_val, generator=generator)}


class RemoteDatasetAdapter(Dataset):
    def __init__(self, dataset, spec: dict, seq_len: int):
        self.dataset = dataset
        self.spec = spec
        self.seq_len = seq_len

    def __len__(self):
        return len(self.dataset)

    def _primary_input(self, sample: dict):
        if "image" in INPUT_MODALITIES:
            value = _extract_image(sample, self.spec.get("image_field"))
            if value is not None:
                return value
        if "audio" in INPUT_MODALITIES:
            value = _extract_audio(sample, self.spec.get("audio_field"))
            if value is not None:
                return value
        if "video" in INPUT_MODALITIES:
            value = _extract_video(sample, self.spec.get("video_field"))
            if value is not None:
                return value
        if "text" in INPUT_MODALITIES:
            value = _extract_text(sample, self.spec.get("text_field"))
            if value is not None:
                return value
        for extractor in (_extract_text, _extract_image, _extract_audio, _extract_video):
            value = extractor(sample)
            if value is not None:
                return value
        label_field = self.spec.get("label_field")
        features = _extract_numeric_features(sample, {label_field} if label_field else None)
        return features if features is not None else sample

    def _pair_inputs(self, sample: dict, idx: int):
        pair = _extract_text_pair(sample, self.spec.get("pair_fields"))
        if pair:
            return pair
        if "image" in INPUT_MODALITIES and "text" in INPUT_MODALITIES:
            image = _extract_image(sample, self.spec.get("image_field"))
            text = _extract_text(sample, self.spec.get("text_field"))
            if image is not None and text is not None:
                return image, text
        other_idx = (idx + 1) % len(self.dataset)
        other = self.dataset[other_idx]
        if isinstance(other, dict):
            return self._primary_input(sample), self._primary_input(other)
        return self._primary_input(sample), other

    def __getitem__(self, idx):
        sample = self.dataset[idx]
        if not isinstance(sample, dict):
            sample = {"value": sample}
        generator = _seeded_generator(idx)
        if TASK_MODE == "pair":
            left, right = self._pair_inputs(sample, idx)
            return (
                _encode_input(left, self.seq_len, generator=generator),
                _encode_input(right, self.seq_len, generator=generator),
            )
        if TASK_MODE == "detection":
            x = _encode_input(self._primary_input(sample), self.seq_len, generator=generator)
            y = _build_detection_target(sample, self.spec, generator=generator)
            return x, y
        if TASK_MODE == "segmentation":
            x = _encode_input(self._primary_input(sample), self.seq_len, generator=generator)
            y = _build_segmentation_target(sample, self.spec, generator=generator)
            return x, y
        if TASK_MODE == "time_series":
            series = sample.get(self.spec.get("series_field", "target"))
            if series is None:
                series = self._primary_input(sample)
            x, y = _build_time_series_pair(series, generator=generator)
            return x, y
        if TASK_MODE == "rl":
            x = _encode_input(self._primary_input(sample), self.seq_len, generator=generator)
            y = _build_rl_target(sample, generator=generator)
            return x, y
        if TASK_MODE == "graph":
            label_val = _extract_label(sample, self.spec.get("label_field"))
            x = _encode_input(self._primary_input(sample), NUM_NODES, generator=generator)
            if HEAD_TYPE == "node_classification_head":
                y = torch.randint(
                    0,
                    max(2, NUM_CLASSES),
                    (NUM_NODES,),
                    dtype=torch.long,
                    generator=generator,
                )
            elif HEAD_TYPE == "link_prediction_head":
                y = torch.randint(
                    0,
                    2,
                    (NUM_NODES, NUM_NODES),
                    dtype=torch.float32,
                    generator=generator,
                )
            elif HEAD_TYPE == "recommendation_head":
                y = _coerce_regression(label_val, NUM_OUTPUTS, generator=generator)
            else:
                y = _coerce_label(label_val, NUM_CLASSES, generator=generator)
            return x, y

        label_val = _extract_label(sample, self.spec.get("label_field"))
        x = _encode_input(self._primary_input(sample), self.seq_len, generator=generator)
        if TASK_MODE == "classification":
            return x, _coerce_label(label_val, NUM_CLASSES, generator=generator)
        if TASK_MODE == "regression":
            return x, _coerce_regression(label_val, NUM_OUTPUTS, generator=generator)
        if TASK_MODE == "language_modeling":
            return x
        return x, x


def available_sample_data() -> list[str]:
    return ["auto", *sorted(SAMPLE_DATASETS.keys())]


def load_data(
    data_path: str = None,
    batch_size: int = 2,
    seq_len: int = {{ blueprint.max_seq_len or 128 }},
    sample_data: str | None = None,
    sample_size: int | None = None,
    dataset: str | None = None,
    dataset_split: str | None = "train",
    dataset_size: int | None = None,
    dataset_config: str | None = None,
    dataset_cache_dir: str | None = None,
):
    """
    Data loader respecting inputs: {{ spec.modality.inputs }}
    Returns a PyTorch DataLoader (or a generator if PyTorch is missing).

    Supports:
        - .bin files: Pre-tokenized binary data (uint16)
        - .txt files: Character-level tokenization
        - remote datasets: Hugging Face datasets via --dataset
        - None: Synthetic random data
    """
    if not torch:
        if dataset:
            raise RuntimeError("Remote datasets require PyTorch. Install torch to proceed.")
        print("PyTorch not found, returning placeholder generator")

        def _gen():
            while True:
                yield "synthetic_batch"

        return _gen()

    selections = [bool(data_path), bool(sample_data), bool(dataset)]
    if sum(selections) > 1:
        raise ValueError("Choose only one of data_path, sample_data, or dataset.")

    if sample_size is not None and sample_size <= 0:
        sample_size = None
    if dataset_size is not None and dataset_size <= 0:
        dataset_size = None

    if data_path and os.path.exists(data_path) and os.path.isdir(data_path):
        print("Data path is a directory; using sample dataset instead.")
        data_path = None

    if dataset:
        spec = _resolve_remote_dataset_spec(dataset, dataset_config)
        if spec.get("provider") != "hf":
            raise RuntimeError(f"Unsupported dataset provider: {spec.get('provider')}")
        remote = _load_hf_dataset(spec, dataset_split, dataset_size, dataset_cache_dir)
        dataset_obj = RemoteDatasetAdapter(remote, spec, seq_len)
        if dataset_size is not None:
            dataset_obj = LimitedDataset(dataset_obj, dataset_size)
        return DataLoader(dataset_obj, batch_size=batch_size, shuffle=True)

    if data_path and os.path.exists(data_path):
        if data_path.endswith(".bin"):
            print(f"Loading pre-tokenized data from {data_path}...")
            dataset_obj = BinaryTokenDataset(data_path, seq_len=seq_len)
        else:
            print(f"Loading text data from {data_path}...")
            dataset_obj = FileTextDataset(data_path, seq_len=seq_len)
    else:
        chosen = sample_data or "auto"
        if chosen == "auto":
            chosen = _auto_sample_name()
        if chosen not in SAMPLE_DATASETS:
            print(f"Unknown sample dataset '{chosen}', falling back to auto.")
            chosen = _auto_sample_name()
        print(f"Using sample dataset: {chosen}")
        dataset_obj = SAMPLE_DATASETS[chosen](seq_len)

    if sample_size is not None:
        dataset_obj = LimitedDataset(dataset_obj, sample_size)

    return DataLoader(dataset_obj, batch_size=batch_size, shuffle=True)
